[
  {
    "objectID": "notebooks/lmfit_20160703.html",
    "href": "notebooks/lmfit_20160703.html",
    "title": "mike.depalatis.net",
    "section": "",
    "text": "%matplotlib inline\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.random as npr\nimport lmfit\nplt.style.use(\"ggplot\")\n\n\n# Generate data\nx = np.linspace(0, 10, 100)\ny = np.sin(5*x)*np.exp(-x/2.5)\ny += npr.choice([-1, 1], size=y.shape)*npr.random(size=y.shape)/5\nplt.plot(x, y, '.')\nplt.savefig(\"../img/lmfit/data.svg\", bbox_inches=\"tight\")\n\n\n\n\n\n# Create the model and guess parameters\nmodel = lmfit.models.ExpressionModel(\"ampl * sin((x - x0)*freq) * exp(-x/tau) + offset\")\nparams = model.make_params(ampl=1, x0=0, freq=10, tau=1, offset=0)\nparams[\"offset\"].set(vary=False)\n\n\n# Fit and visually inspect\nfit = model.fit(y, params, x=x)\nfit.plot_fit()\nplt.savefig(\"../img/lmfit/fit-expression-model.svg\", bbox_inches=\"tight\")\n\n\n\n\n\nprint(fit.fit_report())\n\n[[Model]]\n    Model(_eval)\n[[Fit Statistics]]\n    # function evals   = 122\n    # data points      = 100\n    # variables        = 4\n    chi-square         = 1.369\n    reduced chi-square = 0.014\n    Akaike info crit   = -417.018\n    Bayesian info crit = -406.598\n[[Variables]]\n    freq:     4.97964137 +/- 0.034454 (0.69%) (init= 10)\n    tau:      2.60560348 +/- 0.246606 (9.46%) (init= 1)\n    offset:   0 (fixed)\n    ampl:     1.02045744 +/- 0.068242 (6.69%) (init= 1)\n    x0:       0.00020489 +/- 0.012188 (5948.74%) (init= 0)\n[[Correlations]] (unreported correlations are &lt;  0.100)\n    C(tau, ampl)                 = -0.720 \n    C(freq, x0)                  =  0.681 \n    C(ampl, x0)                  =  0.129 \n\n\n\n\n# Example of model subclassing for better reusability\n\nclass DecayingSineModel(lmfit.Model):\n    def __init__(self, *args, **kwargs):\n        def decaying_sine(x, ampl, offset, freq, x0, tau):\n            return ampl * np.sin((x - x0)*freq) * np.exp(-x/tau) + offset\n        super(DecayingSineModel, self).__init__(decaying_sine, *args, **kwargs)\n    \n    def guess(self, data, **kwargs):         \n        params = self.make_params()\n        def pset(param, value):\n            params[\"%s%s\" % (self.prefix, param)].set(value=value)\n        pset(\"ampl\", np.max(data) - np.min(data))\n        pset(\"offset\", np.mean(data))\n        pset(\"freq\", 1)\n        pset(\"x0\", 0)\n        pset(\"tau\", 1)\n        return lmfit.models.update_param_vals(params, self.prefix, **kwargs)\n\n\nmodel = DecayingSineModel()\nparams = model.guess(y, x=x)\nfit = model.fit(y, params, x=x)\n\n\nfit.plot_fit()\nplt.savefig(\"../img/lmfit/fit-decaying-sine-model.svg\", bbox_inches=\"tight\")\n\n\n\n\n\nprint(fit.fit_report())\n\n[[Model]]\n    Model(decaying_sine)\n[[Fit Statistics]]\n    # function evals   = 152\n    # data points      = 100\n    # variables        = 5\n    chi-square         = 1.362\n    reduced chi-square = 0.014\n    Akaike info crit   = -414.513\n    Bayesian info crit = -401.488\n[[Variables]]\n    ampl:     1.01509806 +/- 0.068642 (6.76%) (init= 1.690452)\n    offset:   0.00866496 +/- 0.012058 (139.16%) (init= 0.02785899)\n    freq:     4.98055762 +/- 0.034458 (0.69%) (init= 1)\n    x0:       0.00051399 +/- 0.012254 (2384.24%) (init= 0)\n    tau:      2.62045162 +/- 0.250289 (9.55%) (init= 1)\n[[Correlations]] (unreported correlations are &lt;  0.100)\n    C(ampl, tau)                 = -0.722 \n    C(freq, x0)                  =  0.681 \n    C(ampl, x0)                  =  0.124 \n    C(ampl, offset)              = -0.110 \n\n\n\n\n# Show all parameters and standard error estimates\nfor key in fit.params:\n    print(key, \"=\", fit.params[key].value, \"+/-\", fit.params[key].stderr)\n\nampl = 1.01509805981 +/- 0.0686424802112\noffset = 0.00866496750688 +/- 0.0120582675657\nfreq = 4.98055762006 +/- 0.0344587866965\nx0 = 0.00051399773059 +/- 0.0122549465211\ntau = 2.62045162933 +/- 0.250289489618"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "MathWorld - An excellent resource for looking up math concepts.\nEric Weisstein’s World of Physics - Similar to MathWorld, only with physics.\nHyperPhysics - Another great (though more elementary) physics resource.\nMaxima - An excellent GPL’d computer algebra system. I’d recommend SymPy instead these days, however.\nFalstad Math and Physics Applets - Some excellent applets on math, quantum mechanics, electrodynamics, etc.\nElectromagnetic Waves and Antennas - A freely available book (with inidividual chapters in PDF form).\nEncyclopedia of Laser Physics and Technology"
  },
  {
    "objectID": "resources.html#physics-and-math",
    "href": "resources.html#physics-and-math",
    "title": "Resources",
    "section": "",
    "text": "MathWorld - An excellent resource for looking up math concepts.\nEric Weisstein’s World of Physics - Similar to MathWorld, only with physics.\nHyperPhysics - Another great (though more elementary) physics resource.\nMaxima - An excellent GPL’d computer algebra system. I’d recommend SymPy instead these days, however.\nFalstad Math and Physics Applets - Some excellent applets on math, quantum mechanics, electrodynamics, etc.\nElectromagnetic Waves and Antennas - A freely available book (with inidividual chapters in PDF form).\nEncyclopedia of Laser Physics and Technology"
  },
  {
    "objectID": "resources.html#python",
    "href": "resources.html#python",
    "title": "Resources",
    "section": "Python",
    "text": "Python\n\nNumpy\nScipy\nMatplotlib\nTornado"
  },
  {
    "objectID": "resources.html#restructuredtext",
    "href": "resources.html#restructuredtext",
    "title": "Resources",
    "section": "reStructuredText",
    "text": "reStructuredText\nI previously used reST for notes and as a plaintext format. I’ve now switched over mostly to Markdown, but since reST is used heavily for Pythonic documentation, it’s still good to know.\n\nA ReStructuredText Primer\nQuick reStructuredText\nDirectives"
  },
  {
    "objectID": "resources.html#latex",
    "href": "resources.html#latex",
    "title": "Resources",
    "section": "LaTeX",
    "text": "LaTeX\n\nGetting to grips with Latex - An excellent introduction to LaTeX.\nLaTeX Beamer - A LaTeX style class for creating presentations. Check out the themes gallery.\nLaTeX Letter Format - For those times when you need to actually send a letter, this is how you do it with LaTeX."
  },
  {
    "objectID": "resources.html#linux",
    "href": "resources.html#linux",
    "title": "Resources",
    "section": "Linux",
    "text": "Linux\n\nDebian - The best Linux distribution out there.\nautotut - A guide to using the GNU autotools (autoconf, automake, autoheader).\nGnuplot Not So Frequently Asked Questions - An incredible overview of gnuplot.\nAdvanced Bash-Scripting Guide - Everything you ever needed to know about Bash scripting.\nFilesystem Hierarchy Standard - Information what everything in / on your Linux system actually means."
  },
  {
    "objectID": "resources.html#electronics",
    "href": "resources.html#electronics",
    "title": "Resources",
    "section": "Electronics",
    "text": "Electronics\n\nDangerous Prototypes - Many interesting open source hardware projects\ndirtypcbs.com - Cheap PCB service (China)\nSeeed Studio - Electronics components and cheap PCB fabrication (China)\n\n\nKiCAD\n\nHOWTOs\n\nMaking New Components and Modules (Footprints) in KiCAD\n\n\n\n\nArduino\n\nEthernet\n\nDIY link 1\nDIY link 2"
  },
  {
    "objectID": "blog/make-refactoring-tests-easier.html",
    "href": "blog/make-refactoring-tests-easier.html",
    "title": "Make refactoring tests easier",
    "section": "",
    "text": "Quite often when I see unit tests written by colleagues I’ll see this sort of pattern:\nfrom package.subpackage.blah import foo, bar, baz\nOn the surface there doesn’t seem to be anything wrong with this. But what happens when you decide that foo doesn’t really belong in subpackage let alone blah and that it instead should live in package.other_subpackage? A good IDE like PyCharm, VS Code, or a properly configured Emacs provides you with helpful refactoring tools that will adjust your imports to read something like\nfrom package.subpackage.blah import bar, baz\nfrom package.other_subpackage.fizz import foo\nThis is a good start but to keep things properly organized you’ll likely need to manually move the unit test code to an appropriately named module, too. When I write unit tests if I have a package structure like\npackage\n|__ __init__.py\n|__ subpackage\n    |__ __init__.py\n    |__ blah.py\n|__ other_subpackage\n    |__ __init__.py\n    |__ fizz.py\nI like to lay out my test modules with a one-to-one correspondence with the package’s module:\ntest_package\n|__ __init__.py\n|__ test_subpackage\n    |__ __init__.py\n    |__ test_blah.py\n|__ test_other_subpackage\n    |__ __init__.py\n    |__ test_fizz.py\nSo in this case I want to move my tests for foo into test_fizz.py. For a single test case this is not a big deal, but with more things being moved around it can get tricky. One thing I’ve found that makes it exceedingly easy to move code around is in unit tests import the module under test as module:\nfrom package.subpackage import blah as module\nNow all your tests will refer to foo as module.foo so when you move those around to somewhere else all you need to do is have a similar import statement at the top of your test module.\nThis may seem like a small thing but I’ve found it quite helpful. Another useful feature of importing the module instead of each part individually is that as your modules grow you don’t need to have giant import statements: everything is already there under the module namespace."
  },
  {
    "objectID": "blog/postgres-time-series-database.html",
    "href": "blog/postgres-time-series-database.html",
    "title": "Using Postgres as a time series database",
    "section": "",
    "text": "Time series databases (TSDBs) are quite popular these days. To name a few, there are InfluxDB, Graphite, Druid, Kairos, and Prometheus. All aim to optimize data storage and querying for time-based data, which is highly relevant in a physics labs where there are multitude of “metrics” (to borrow a phrase used frequently in TSDB documentation) that naturally lend themselves to time series representation: lab (and individual device) temperatures, vacuum chamber pressures, and laser powers, just to name a few. Ideally, one could log various data to one of these databases and then use a tool like Grafana to visualize it. Sadly, more traditional relational databases like SQLite and PostgreSQL are not (currently) supported by Grafana (although this is now being addressed by a datasource plugin in development).\nNevertheless, there are quite a few reasons to favor a traditional RDBMS over a newfangled TSDB. To name a few:\nIn this post, I will outline a few things I have learned in using SQL for storing time series data. In particular, I will focus on Postgres, but the same general principles apply to other dialects. Sample code for some examples can be found on GitLab."
  },
  {
    "objectID": "blog/postgres-time-series-database.html#schema-definition",
    "href": "blog/postgres-time-series-database.html#schema-definition",
    "title": "Using Postgres as a time series database",
    "section": "Schema definition",
    "text": "Schema definition\nOne “disadvantage” to SQL is it traditionally requires tightly defined schema. In practice when logging time series data, this is not usually a problem since each measurement device can neatly have its own column. Where this can become somewhat of nuiscance is when adding new devices. InfluxDB (for example) gets around this with its query language being quite flexible. In traditional SQL, the approach would require altering a table to add a new column. This is not too difficult in principle, but requires a (naive) program for logging data to frequently make ALTER TABLE calls and check if columns already exist. (Note that if using Python, this can be easily dealt with by using the dataset library.).\nIn real laboratories, though, we tend to know the kinds of things we are going to measure. So even if we add new devices that we want to log data from, we can still come up with a reasonable schema definition that fits well within the SQL paradigm. As an example, let’s consider storing data from thermocouples in a table. We could get away with as few as three columns to describe the data: a timestamp (of course), a name or unique ID of the sensor, and a temperature measurement. For good measure, we should also add a primary key ID column to make a grand total of four columns. So far, our table looks like this:\n id |           timestamp           |  sensor   | temperature\n----+-------------------------------+-----------+-------------\nFor the timestamp column, I highly recommend using TIMESTAMP WITH TIME ZONE rather than TIMESTAMP WITHOUT TIME ZONE (more on why later).\nFor efficient querying, we’ll want to index the timestamp and sensor columns. Depending on the number of sensors, it may also make sense to make a combined index on both, but we can defer this decision to later if it becomes necessary. Using SQLAlchemy, we define our table like this:\nmetadata = sa.MetaData()\ntable = sa.Table(\n    'timeseries', metadata,\n    sa.Column('id', sa.Integer, primary_key=True),\n    sa.Column('timestamp', sa.DateTime(timezone=True),\n              nullable=False, index=True),\n    sa.Column('sensor', sa.String(length=128), nullable=False, index=True),\n    sa.Column('temperature', sa.Float(precision=4), nullable=False))\nmetadata.create_all(bind=engine)\nwhich results in the following SQL:\nCREATE TABLE timeseries (\n    id SERIAL NOT NULL,\n    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,\n    sensor VARCHAR(128) NOT NULL,\n    temperature FLOAT(4) NOT NULL,\n    PRIMARY KEY (id)\n);\nCREATE INDEX ix_timeseries_sensor ON timeseries (sensor);\nCREATE INDEX ix_timeseries_timestamp ON timeseries (timestamp);"
  },
  {
    "objectID": "blog/postgres-time-series-database.html#basic-querying",
    "href": "blog/postgres-time-series-database.html#basic-querying",
    "title": "Using Postgres as a time series database",
    "section": "Basic querying",
    "text": "Basic querying\nSimple queries are performed as normal:\nSELECT * FROM timeseries WHERE sensor = 'sensor_01';\nPostgres has quite a few date and time functions for building more complicated queries. It understands ISO 8601 out of the box:\ntest=&gt; SELECT * FROM timeseries\ntest-&gt; WHERE timestamp &gt; '2016-06-13T22:00+02'\ntest-&gt; AND sensor = 'sensor_01';\n id |           timestamp           |  sensor   | temperature\n----+-------------------------------+-----------+-------------\n  8 | 2016-06-14 23:18:16.149606+02 | sensor_01 |     22.7061\n  4 | 2016-06-14 23:18:11.985645+02 | sensor_01 |     25.4643\n(2 rows)\nHere we are explicit with the UTC offset of +2 hours (CEST). If omitted, the server locale is assumed. This brings us to why we should bother with time zones in the first place: internally, we want all timestamps stored in UTC to avoid ambiguity (Postgres already does this internally). Externally, (e.g., from Python scripts), we want to be able to use whatever time zone we’re in to not have to think too hard.\nSQLAlchemy treats naive datetime objects, uh, naively. This means that if a new datetime is created without explicitly specifying a time zone, that +2 hours above is lost and our time quries will start to get confusing. To avoid this problem, the best solution I have found is to always declare columns as TIMESTAMP WITH TIME ZONE (DateTime(timezone=True) in SQLAlchemy terms) and explicitly. So rather than inserting new timestamps with\nfrom datetime import datetime\n# ...\ntimestamp = datetime.now()\ninstead prefer\nfrom datetime import datetime, timezone\n# ...\ntimestamp = datetime.now(timezone.utc)\nAside: why oh why doesn’t datetime.utcnow just do this?\nNow we can build queries in Python like this using pandas and raw SQL queries:\ntoday = datetime.now(timezone.utc)\ntoday = today.replace(hour=0, minute=0, second=0, microsecond=0)\nquery = (\n    \"SELECT * FROM timeseries \" +\n    \"WHERE timestamp &gt;= '{}' \".format(today.isoformat()) +\n    \"AND sensor = 'sensor_01'\"\n)\ndf = pd.read_sql_query(query, engine, index_col=\"timestamp\")"
  },
  {
    "objectID": "blog/postgres-time-series-database.html#data-aggregation",
    "href": "blog/postgres-time-series-database.html#data-aggregation",
    "title": "Using Postgres as a time series database",
    "section": "Data aggregation",
    "text": "Data aggregation\nDepending on data density, it may be useful to downsample data and look at aggregates such as the mean temperature in half-hour windows over the course of a day. We can easily accomplish this after the fact with pandas, but we can just as easily use Postgres aggregate functions to do this for us on the server. One advantage to this approach is a reduction in network overhead, which is especially relevant for very large datasets. Another is that these queries can be cached using materialized views. (This is a more advanced topic that I will not cover here. Instead, see the link in the references section below for a good treatment).\nThe key here is to use the date_trunc aggregate function and GROUP BY to only look at (for example) one hour at a time. An example of an aggregate query:\nSELECT\n  date_trunc('hour', timestamp) AS timestamp,\n  avg(temperature) AS temperature\nFROM timeseries\nWHERE timestamp &gt;= '2016-06-25'\nAND sensor = 'sensor_01'\nGROUP BY date_trunc('hour', timestamp)\nORDER BY timestamp;\nwhich results in something like:\ntimestamp        |   temperature\n------------------------+------------------\n2016-06-25 00:00:00+02 | 22.0828623312065\n2016-06-25 01:00:00+02 | 22.0026334276975\n2016-06-25 02:00:00+02 | 21.9871146672498\n2016-06-25 03:00:00+02 | 22.0274553065207\n2016-06-25 04:00:00+02 | 21.9357200048187\n2016-06-25 05:00:00+02 | 21.9737668623899\n2016-06-25 06:00:00+02 | 22.0098525849685\n2016-06-25 07:00:00+02 | 22.0767008988982\n2016-06-25 08:00:00+02 | 22.2146511332874\n2016-06-25 09:00:00+02 | 21.9118559617263\n2016-06-25 10:00:00+02 | 22.0417969508838\n2016-06-25 11:00:00+02 | 22.0554379473676\n2016-06-25 12:00:00+02 | 22.0193907419841\n2016-06-25 13:00:00+02 | 22.0560295554413\n2016-06-25 14:00:00+02 | 21.8087244594798\n2016-06-25 15:00:00+02 | 22.0494429762518\n2016-06-25 16:00:00+02 | 21.9082782661007\n2016-06-25 17:00:00+02 | 21.4403478373652\n(18 rows)"
  },
  {
    "objectID": "blog/postgres-time-series-database.html#other-strategies",
    "href": "blog/postgres-time-series-database.html#other-strategies",
    "title": "Using Postgres as a time series database",
    "section": "Other strategies",
    "text": "Other strategies\nAnother approach to avoid the time zone issue entirely is to simply store timestamps using something like UNIX time. Since pretty much every programming language imaginable has a built-in function to return time in seconds since the epoch, this is a reasonable approach (and is a bit more portable to other SQL dialects). The major downside to this is that compared to ISO 8601, UNIX time is not as readable by humans and therefore may require extra steps to convert to and from a human-readable format.\nDepending on what you are doing with your data, it could also make sense to store, say, an hour’s worth of data in a single row using the ARRAY data type. Combining arrays with array functions could then effectively do aggregation (somewhat) automatically rather than by query. This could also mean a bit of extra work when inserting new data or getting data stored in the database into a form friendly to your data analysis tools of choice."
  },
  {
    "objectID": "blog/postgres-time-series-database.html#references-and-further-reading",
    "href": "blog/postgres-time-series-database.html#references-and-further-reading",
    "title": "Using Postgres as a time series database",
    "section": "References and further reading",
    "text": "References and further reading\n\nQuerying Time Series in Postgresql\nMaterialized View Strategies Using PostgreSQL"
  },
  {
    "objectID": "blog/lmfit.html",
    "href": "blog/lmfit.html",
    "title": "Fitting with lmfit",
    "section": "",
    "text": "General-purpose fitting in Python can sometimes be a bit more challenging than one might at first suspect given the robust nature of tools like Numpy and Scipy. First we had leastsq. It works, although often requires a bit of manual tuning of initial guesses and always requires manual calculation of standard error from a covariance matrix (which isn’t even one of the return values by default). Later we got curve_fit which is a bit more user friendly and even estimates and returns standard error for us by default! Alas, curve_fit is just a convenience wrapper on top of leastsq and suffers from some of the same general headaches.\nThese days, we have the wonderful lmfit package. Not only can lmfit make fitting more user friendly, but it also is quite a bit more robust than using scipy directly. The documentation is thorough and rigorous, but that can also mean that it can be a bit overwhelming to get started with it. Here I work through a basic example in two slightly different ways in order to demonstrate how to use it."
  },
  {
    "objectID": "blog/lmfit.html#generating-the-data",
    "href": "blog/lmfit.html#generating-the-data",
    "title": "Fitting with lmfit",
    "section": "Generating the data",
    "text": "Generating the data\nLet’s assume we have data that resembles a decaying sine wave (e.g., a damped oscillator). lmfit has quite a few pre-defined models, but this is not one of them. We can simulate the data with the following code:\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(5*x)*np.exp(-x/2.5)\nReal data is noisy, so let’s add some noise:\nimport numpy.random as npr\n\ny += npr.choice([-1, 1], size=y.shape)*npr.random(size=y.shape)/5\nThe resulting data:\n\n\n\nDecaying sine generated data"
  },
  {
    "objectID": "blog/lmfit.html#using-models",
    "href": "blog/lmfit.html#using-models",
    "title": "Fitting with lmfit",
    "section": "Using models",
    "text": "Using models\nThe easiest way to work with lmfit is to ignore the lmfit.minimize function shown in the “Getting Started” section of the documentation and instead jump straight to the higher-level (and more useful) Model class. For one-time fitting, the lmfit.models.ExpressionModel class is provided. When creating a new ExpressionModel, you simply pass a string that is interpreted as a Python expression. For our decaying sine example, we might do this:\nimport lmfit\n\nmodel = lmfit.models.ExpressionModel(\"ampl * sin((x - x0)*freq) * exp(-x/tau) + offset\")\nLet’s make our initial guess for performing the fit under the constraint that the offset is fixed at 0:\nparams = model.make_params(ampl=1, x0=0, freq=10, tau=1, offset=0)\nparams[\"offset\"].set(vary=False)\nTo fit, we pass the data and the parameters as arguments and the independent variable as a keyword argument:\nfit = model.fit(y, params, x=x)\nTo visually check if the fit is good, lmfit provides both plot_fit and plot_residuals methods for model instances. The former shows the data, the initial guess, and its found best fit:\n\n\n\nResult of the ExpressionModel fit\n\n\nWe can also see the found parameters with standard errors and goodness of fit data with a fit report (print(model.fit_report())):\n[[Model]]\n    Model(_eval)\n[[Fit Statistics]]\n    # function evals   = 102\n    # data points      = 100\n    # variables        = 4\n    chi-square         = 1.337\n    reduced chi-square = 0.014\n    Akaike info crit   = -419.379\n    Bayesian info crit = -408.959\n[[Variables]]\n    ampl:     1.02147340 +/- 0.068013 (6.66%) (init= 1)\n    offset:   0 (fixed)\n    tau:      2.53669407 +/- 0.239335 (9.43%) (init= 1)\n    x0:      -0.00823894 +/- 0.012256 (148.76%) (init= 0)\n    freq:     4.98932400 +/- 0.035399 (0.71%) (init= 10)\n[[Correlations]] (unreported correlations are &lt;  0.100)\n    C(ampl, tau)                 = -0.718\n    C(x0, freq)                  =  0.684\n    C(ampl, x0)                  =  0.139"
  },
  {
    "objectID": "blog/lmfit.html#reusable-models",
    "href": "blog/lmfit.html#reusable-models",
    "title": "Fitting with lmfit",
    "section": "Reusable models",
    "text": "Reusable models\nFor improved reusability of models, a better approach is to subclass lmfit.models.Model directly. This allows us to implement a guess method to automate creating initial guesses. Following the pattern used in defining the models in the lmfit.models module, we can define our decaying sine model like so:\nclass DecayingSineModel(lmfit.Model):\n    def __init__(self, *args, **kwargs):\n        def decaying_sine(x, ampl, offset, freq, x0, tau):\n            return ampl * np.sin((x - x0)*freq) * np.exp(-x/tau) + offset\n        super(DecayingSineModel, self).__init__(decaying_sine, *args, **kwargs)\n\n    def guess(self, data, **kwargs):\n        params = self.make_params()\n        def pset(param, value):\n            params[\"%s%s\" % (self.prefix, param)].set(value=value)\n        pset(\"ampl\", np.max(data) - np.min(data))\n        pset(\"offset\", np.mean(data))\n        pset(\"freq\", 1)\n        pset(\"x0\", 0)\n        pset(\"tau\", 1)\n        return lmfit.models.update_param_vals(params, self.prefix, **kwargs)\nNote that the point of the prefix is so that composite models can be constructed (the prefix prevents namespace clashes). Now we can fit as before but guess the starting parameters without thinking about it:\nmodel = DecayingSineModel()\nparams = model.guess(y, x=x)\nfit = model.fit(y, params, x=x)\nwhich results in a similar fit as before:\n\n\n\nResult of the DecayingSineModel fit"
  },
  {
    "objectID": "blog/lmfit.html#extracting-data-from-the-fit",
    "href": "blog/lmfit.html#extracting-data-from-the-fit",
    "title": "Fitting with lmfit",
    "section": "Extracting data from the fit",
    "text": "Extracting data from the fit\nIn many cases we might want to extract parameters and standard error estimates programatically rather than by reading the fit report (e.g., if the fit will be used to produce a data point on another plot, then the standard error can be used for computing error bars). This is all included in the fit result via its params attribute. We can print the parameter values and errors like this:\nfor key in fit.params:\n    print(key, \"=\", fit.params[key].value, \"+/-\", fit.params[key].stderr)"
  },
  {
    "objectID": "blog/lmfit.html#final-thoughts",
    "href": "blog/lmfit.html#final-thoughts",
    "title": "Fitting with lmfit",
    "section": "Final thoughts",
    "text": "Final thoughts\nI’ve only scratched the surface of lmfit’s features, but the examples here demonstrate a good portion of the daily requirements of working with data from an experiment. As alluded to earlier, lmfit comes with many built-in models which makes it a pleasure to use for peak fitting (something that is often particularly difficult when using scipy directly).\nFinally, although lmfit can handle linear models just fine, I would instead recommend the statsmodels package. Using the power of pandas DataFrames, models can be defined in a similar manner as with lmfit’s ExpressionModels.\nA Jupyter notebook containing the above examples can be found here."
  },
  {
    "objectID": "blog/flask-sse-demo.html",
    "href": "blog/flask-sse-demo.html",
    "title": "Flask and server-sent events",
    "section": "",
    "text": "I recently discovered the existence of the HTML5 server-sent events standard. Although it lacks the bidirectional communications of a websocket, SSE is perfect for the publish-subscribe networking pattern. This pattern just so happens to fit in conveniently with writing software to remotely monitor hardware that many people might want to check in on at the same time.\nIn order to try SSE out within a Flask framework, I put together a simple demo app using gevent. The core of the demo on the Python side looks like this:\napp = Flask(__name__)\n\ndef event():\n    while True:\n        yield 'data: ' + json.dumps(random.rand(1000).tolist()) + '\\n\\n'\n        gevent.sleep(0.2)\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/stream/', methods=['GET', 'POST'])\ndef stream():\n    return Response(event(), mimetype=\"text/event-stream\")\nThis can be run either using gevent’s WSGI server or gunicorn using gevent workers.\nUpdate 2016-04-21: There is now a very nice Flask extension called Flask-SSE which handles all of this for you. It additionally supports the concept of channels in order to fine tune what notifications a given client receives."
  },
  {
    "objectID": "blog/conda-shortcuts.html",
    "href": "blog/conda-shortcuts.html",
    "title": "Useful BASH shortcuts for conda",
    "section": "",
    "text": "Anaconda’s conda has become the de facto environment management tool in Python-oriented scientific computing. Its lightweight environments are suitable for development and, at least in some cases, deployment. In general, for each new project, I like to create a new environment. Creating a new, mostly empty environment is done with the following command:\n$ conda env create -y -n my-project-name\nThen the new environment must be activated with\n$ conda activate my-project-name\nIf I later want to remove the environment:\n$ conda env remove -y -n my-project-name\nThis can get tiresome, so I have the following functions defined in my .bashrc:\nfunction conact() {\n    conda activate $(basename $(pwd))\n}\n\nfunction cenv() {\n    if [ -f environment.yaml ]; then\n    conda env create --file=environment.yaml -n $(basename $(pwd))\n    else\n    conda create -yn $(basename $(pwd))\n    fi\n}\n\nfunction rmcenv() {\n    conda remove -n $(basename $(pwd)) --all\n}\nNow when starting a new project, I need only type\n$ cenv\n$ conact"
  },
  {
    "objectID": "blog/multiline-lambdas.html",
    "href": "blog/multiline-lambdas.html",
    "title": "Multiline lambdas",
    "section": "",
    "text": "Although Python has anonymous lambda functions, they lack the flexibility that some languages such as Javascript or even modern C++ have. In Python, lambda functions are limited to a single statement, which is often interpreted as meaning that it can only do one thing. This is not strictly true, however, since constructing a tuple is considered a single statement. In other words, we can cheat a little and call two independent functions in one lambda like this:\n(lambda: (foo(), bar()))()\nThis is still somewhat constraining since variables cannot be defined within the lambda expression, so cases where this trick is useful are limited. One instance where this is particularly nice though is when defining callbacks for a GUI. Typically when a user clicks on a button, there might be several actions that should be triggered, such as starting an experiment, updating a GUI label, etc. Below is a simple example to illustrate this method:\nimport sys\nfrom PyQt5.QtWidgets import *\nfrom PyQt5.QtGui import *\n\n\nclass MainWindow(QWidget):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.setWindowTitle('Multiline lambdas')\n\n        self.label = QLabel(\"Click the button\")\n\n        self.button = QPushButton(\"Click me!\")\n        self.button.clicked.connect(lambda: (\n            self.button.setEnabled(False),\n            self.button.setText(\"You clicked me!\"),\n            self.label.setText(\"Woo!\")\n        ))\n\n        layout = QVBoxLayout()\n        layout.addWidget(self.label)\n        layout.addWidget(self.button)\n        self.setLayout(layout)\n\n\nif __name__ == \"__main__\":\n    app = QApplication(sys.argv)\n    win = MainWindow()\n    win.show()\n    sys.exit(app.exec_())"
  },
  {
    "objectID": "blog/custom-backends-for-cachetools.html",
    "href": "blog/custom-backends-for-cachetools.html",
    "title": "Custom backends for cachetools",
    "section": "",
    "text": "cachetools is a popular library for Python that provides a number of caching tools. In addition to providing decorators to easily memoize function and method calls it provides implementations for several common cache invalidation algorithms. While the cache decorators can take an arbitrary MutableMapping as the cache backend, the cache classes currently only work with an in-memory dictionary stored as a name-mangled __data attribute. Although there is an issue logged for this, the maintainer doesn’t seem to be interested in changing this. Of course it is always fine for an open source maintainer to choose which issues to address and which to leave as is but I often want to be able to use a cache backed by something other than an in-memory dict.\nLuckily there is an easy workaround by consulting the Python documentation on private name mangling. Here is a simple example:\nfrom cachetools import Cache\n\n\nclass MyBackend(dict):\n    def __repr__(self):\n        return \"hi there, I'm a custom backend\"\n\n\nclass MyCache(Cache):\n    def __init__(self):\n        super().__init__(maxsize=10)\n        self._Cache__data = MyBackend()\n\n\ncache = MyCache()\ncache[\"foo\"] = \"foo\"\ncache[\"bar\"] = \"bar\"\nprint(cache[\"foo\"])\nprint(cache[\"bar\"])\nprint(cache)\nRunning this script results in the following output:\nfoo\nbar\nMyCache(hi there, I'm a custom backend, maxsize=10, currsize=2)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Articles",
    "section": "",
    "text": "TIL: Handy VS Code options\n\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n  \n\n\n\n\nCustom backends for cachetools\n\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\n\n\n\n\n  \n\n\n\n\nMake refactoring tests easier\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nUsing ART with Wayland\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nEnsuring timestamp storage in UTC with SQLAlchemy\n\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2019\n\n\n\n\n\n\n  \n\n\n\n\nUseful BASH shortcuts for conda\n\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2018\n\n\n\n\n\n\n  \n\n\n\n\nJupyter integration with conda environments\n\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2018\n\n\n\n\n\n\n  \n\n\n\n\nMultiline lambdas\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2018\n\n\n\n\n\n\n  \n\n\n\n\nSharing data between processes with SQLite\n\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2017\n\n\n\n\n\n\n  \n\n\n\n\nSimplifying argparse usage with subcommands\n\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2016\n\n\n\n\n\n\n  \n\n\n\n\nJavascript for Python programmers\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2016\n\n\n\n\n\n\n  \n\n\n\n\nGetting Matplotlib’s colors in order\n\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2016\n\n\n\n\n\n\n  \n\n\n\n\nFitting with lmfit\n\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2016\n\n\n\n\n\n\n  \n\n\n\n\nUsing Postgres as a time series database\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2016\n\n\n\n\n\n\n  \n\n\n\n\nImporting one Mercurial repository into another\n\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2015\n\n\n\n\n\n\n  \n\n\n\n\nRunning (possibly) blocking code like a Tornado coroutine\n\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2015\n\n\n\n\n\n\n  \n\n\n\n\nBackground tasks with Tornado\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2015\n\n\n\n\n\n\n  \n\n\n\n\nFlask and server-sent events\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2015\n\n\n\n\n\n\n  \n\n\n\n\nUsing websockets with Flask via Tornado\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2014\n\n\n\n\n\n\n  \n\n\n\n\nssh-agent for sudo authentication with a passwordless account\n\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2014\n\n\n\n\n\n\n  \n\n\n\n\nRaspberry Pi as a USB to Ethernet Gateway\n\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/sudo-ssh-auth.html",
    "href": "blog/sudo-ssh-auth.html",
    "title": "ssh-agent for sudo authentication with a passwordless account",
    "section": "",
    "text": "For best security on a public system, it is generally best to disable password-based logins with ssh and instead require authorized keys. However, this complicates things if you want to use sudo with a regular user account, since by default it uses the standard system password to verify the user is authorized to run commands as root.\nEnter pam_ssh_agent_auth. This module allows using regular ssh keys and ssh-agent to verify the user has the proper authorization to use sudo."
  },
  {
    "objectID": "blog/sudo-ssh-auth.html#prerequisites",
    "href": "blog/sudo-ssh-auth.html#prerequisites",
    "title": "ssh-agent for sudo authentication with a passwordless account",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou’ll want to start by ensuring you have generated ssh keys for your user and are using ssh-agent. To generate the keys:\n$ ssh-keygen\nThen just accept the defaults, but make sure to set a password for your new key pair. Add the public key to $HOME/.ssh/authorized_keys."
  },
  {
    "objectID": "blog/sudo-ssh-auth.html#installation",
    "href": "blog/sudo-ssh-auth.html#installation",
    "title": "ssh-agent for sudo authentication with a passwordless account",
    "section": "Installation",
    "text": "Installation\nSince the PAM module isn’t in Debian, first grab the build dependencies:\n# apt-get install build-essential checkinstall libssl-dev libpam0g-dev\nNext, grab the source and build:\n# wget http://downloads.sourceforge.net/project/pamsshagentauth/pam_ssh_agent_auth/v0.10.2/pam_ssh_agent_auth-0.10.2.tar.bz2\n# tar -xvjf pam_ssh_agent_auth-0.10.2.tar.bz2\n# cd pam_ssh_agent_auth-0.10.2\n# ./configure --libexecdir=/lib/security --with-mantype=man\n# make\n# checkinstall\nNote that the libexecdir option to the configure script is set since apparently Debian keeps PAM modules in a different place than pam_ssh_agent_auth expects by default."
  },
  {
    "objectID": "blog/sudo-ssh-auth.html#configuration",
    "href": "blog/sudo-ssh-auth.html#configuration",
    "title": "ssh-agent for sudo authentication with a passwordless account",
    "section": "Configuration",
    "text": "Configuration\nEdit the file /etc/pam.d/sudo and add the following line before any other auth or @include commands:\nauth sufficient pam_ssh_agent_auth.so file=~/.ssh/authorized_keys\nRun visudo to edit /etc/sudoers and add this line before any other Defaults lines:\nDefaults env_keep += SSH_AUTH_SOCK"
  },
  {
    "objectID": "blog/sudo-ssh-auth.html#invoking-sudo",
    "href": "blog/sudo-ssh-auth.html#invoking-sudo",
    "title": "ssh-agent for sudo authentication with a passwordless account",
    "section": "Invoking sudo",
    "text": "Invoking sudo\nTo actually be able to use sudo now, run ssh-agent like so:\n$ eval `ssh-agent`\nand add the key:\n$ ssh-add -t 600\nThis will set the keys to timeout in 10 minutes (600 seconds)."
  },
  {
    "objectID": "blog/sudo-ssh-auth.html#todo",
    "href": "blog/sudo-ssh-auth.html#todo",
    "title": "ssh-agent for sudo authentication with a passwordless account",
    "section": "TODO",
    "text": "TODO\nA more elegant way of adding keys and running ssh-agent, including checking to see if a process is already running!"
  },
  {
    "objectID": "blog/sudo-ssh-auth.html#references",
    "href": "blog/sudo-ssh-auth.html#references",
    "title": "ssh-agent for sudo authentication with a passwordless account",
    "section": "References",
    "text": "References\n\nHow to allow authentication with sudo using an alternate password?\nUsing SSH agent for sudo authentication\nUsing ssh-agent with ssh"
  },
  {
    "objectID": "blog/javascript-for-python-programmers.html",
    "href": "blog/javascript-for-python-programmers.html",
    "title": "Javascript for Python programmers",
    "section": "",
    "text": "Unless you’re just writing a simple HTTP API server, any amount of web programming in Python will likely require at least a little bit of Javascript. Like it or not (and I will try to argue in this post that you should like it for what it’s good at), Javascript is really the only game in town when it comes to client-side scripting on a web page. Sure, there are a number of Python-to-Javascript transpilers out there, but using these just tends to limit the ability to use new Javascript features as they are rolled out to browsers and may limit the ability to use third-party Javascript libraries. At the very least, using one these transpilers introduces added complexity to deploying a web app1.\nIn this post, I will describe some things I’ve learned about Javascript from the perspective of someone who prefers to use Python as much as possible. This guide is mainly aimed at scientists and others who are not primarily programmers but who may find it useful to make a web app for their main work. It is assumed that the reader is at least moderately familiar with Javascript (Mozilla has a nice tutorial to get you up to speed if not)."
  },
  {
    "objectID": "blog/javascript-for-python-programmers.html#namespaces-encapsulation-modularization-and-bundling",
    "href": "blog/javascript-for-python-programmers.html#namespaces-encapsulation-modularization-and-bundling",
    "title": "Javascript for Python programmers",
    "section": "Namespaces, encapsulation, modularization, and bundling",
    "text": "Namespaces, encapsulation, modularization, and bundling\nModules in Python make it very easy to encapsulate components without polluting the global namespace. In contrast, Javascript in the browser will make everything a global if you are not careful2. The good news is that it doesn’t require too much extra effort to use a sort of module pattern in your Javascript code thanks to things like object literals and closures. Imagine you are writing some code to do some simple math operations that aren’t in the Javascript Math library. Instead of doing this:\n// Don't do this\nfunction mean(x) {\n  var i;\n  var total = 0;\n  for (i = 0; i &lt; x.length; i++) {\n    total += x[i];\n    }\n  return total/x.length;\n}\nyou should prefer putting your functions in an object:\n// This is better\nvar mathlib = {\n  mean: function (x) {\n    var i;\n    var total = 0;\n    for (i = 0; i &lt; x.length; i++) {\n      total += x[i];\n    }\n    return total/x.length;\n  }\n};\nAn even better approach is to wrap your definitions in a closure to keep things better encapsulated:\n// This is even better!\nvar mathlib = (function (lib) {\n  lib.mean = function (x) {\n    var i;\n    var total = 0;\n    for (i = 0; i &lt; x.length; i++) {\n      total += x[i];\n    }\n    return total/x.length;\n  };\n\n  return lib;\n})(mathlib || {});\nThis pattern allows for splitting components for a larger module into different files, which is often a good idea from the perspective of readability when things start getting more complex. Rather than go into further detail, I’ll refer you to an excellent article on the module pattern in Javascript by Ben Cherry."
  },
  {
    "objectID": "blog/javascript-for-python-programmers.html#bundling",
    "href": "blog/javascript-for-python-programmers.html#bundling",
    "title": "Javascript for Python programmers",
    "section": "Bundling",
    "text": "Bundling\nHTTP/1.1 requires a new connection for every requested Javascript file. While this problem is rectified in HTTP/2, not many web servers and hosting providers support it yet as of late 2016. This has led to many different options for bundling multiple Javascript files into a single file which can be included in a web page with just one script tag. While these tools can be tempting to use, I strongly recommend avoiding them as much as possible (at least until you become more comfortable with the state of modern Javascript) for the following reasons:\n\nThey require having Node.js installed. If you’ve read this far, you can probably handle that, but if your scientist colleagues aren’t as experienced as you are with software development, asking them to have two entirely different language runtimes installed just to run the latest version of your helpful web interface may be asking a bit too much.\nThe churn in Javascriptland is too fast-paced for those of us who have other things to spend our time on (although from my outsider’s perspective, things appear to be settling down a bit lately).\nBundling files makes Javascript debugging a bit more difficult insofar as it usually requires also building source maps. While most of the Javascript bundlers will do this for you if asked, it just adds to the overall complexity.\nThe module pattern presented above is already sufficient in many cases. After all, what’s a few milliseconds to load a second Javascript file from a server on your LAN?\n\nA good, (potentially) pure Python approach to bundling your Javascript files (for cases where it makes sense to split code into more than a single file) is the webassets module. Webassets offers a number of filters to run Javascript and CSS files through to accomplish tasks such as as minification and bundling. Here’s a sample Tornado app:\nimport tornado.web\nimport tornado.ioloop\nfrom webassets import Environment\n\n\nclass MainHandler(tornado.web.RequestHandler):\n    def initialize(self, assets):\n        self.assets = assets\n\n    def get(self):\n        self.render(\"index.html\", assets=assets)\n\n\n# Set up the webassets environment and make a bundle\nassets = Environment(directory=\".\", url=\"/static\")\njs_files = [\"mathlib.js\", \"thing.js\", \"class-example.js\"]\nassets.register(\"bundle\", *js_files,\n                output=\"bundled.min.js\",\n                filters=\"rjsmin\")  # webassets ships with this filter included\n\napp = tornado.web.Application(\n    [(r'/', MainHandler, dict(assets=assets))],\n    static_path=\".\",\n    template_path=\".\",\n    debug=True)\n\napp.listen(8123)\ntornado.ioloop.IOLoop.current().start()\nTo include the bundled file in the template, you would do something like this in the template:\n{% for url in assets['bundle'].urls() %}\n  &lt;script src=\"{{ url }}\"&gt;&lt;/script&gt;\n{% end %}\nThe careful reader may wonder why the for loop is used if all the Javascript files will be bundled into a single file in the end. This is because webassets has a helpful debug mode which eliminates the need for source mapping. By adding assets.debug = True to the Python file, assets['bundle'].urls() will return a list of all the original, uncompressed Javascript files. This results in individual script tags for each Javascript source file which makes debugging in the browser considerably easier at the expense of a (likely) small increase in startup time.\nThere are a lot of nice features in webassets, though many of the filters require third-party tools (often using Node.js) to be installed. For this reason, I discourage using most of these until and unless you are comfortable with the rabbit hole of the Node world.\n(Aside: Recently, I learned of the DukPy interpreter. While it’s still early, it looks like a promising way of being able to include things that currently require Node-based tools while keeping everything purely Pythonic.)"
  },
  {
    "objectID": "blog/javascript-for-python-programmers.html#embracing-the-present",
    "href": "blog/javascript-for-python-programmers.html#embracing-the-present",
    "title": "Javascript for Python programmers",
    "section": "Embracing the present",
    "text": "Embracing the present\nAmong Python programmers, Javascript has a tendency to be considered a very poor programming language in terms of features and syntax. While this was once a more valid criticism, Javascript has steadily improved, and especially with the introduction of the ES2015 standard, it’s a very comfortable language to work in3. In this section, I will cover a few of the more useful features made available in ES2015 with the small caveat that using them requires using fairly up-to-date browsers (which is not normally a problem among scientists who in my experience are all using either Firefox or Chrome, anyway).\n\nArrow functions\nIn Python, self by convention refers to the instance of a class. This means that even with nested function definitions, the reference to self is always unambiguous, so you could do something like\nclass Thing:\n    def __init__(self, value):\n        self.value = value\n\n    def method(self):\n        def function():\n            return self.value\n        return function()\nand expect calling the method method on an instance of Thing to correctly return thing.value. In Javascript, the approximate equivalent of self is this which is by default bound as a reference to the function being called. In other words,\nvar thing = {\n  value: 1,\n  method: function () {\n    var func = function () {\n      return this.value;\n    };\n    return func();\n  }\n};\n\nconsole.log(thing.method());\nwill print undefined because func has no value attribute. This can be fixed by explicitly binding this to func (func = func.bind(this)), but this quickly becomes cumbersome when the number of functions that need this fix grows. In part to simplify this, ES2015 introduced so-called arrow functions which are kind of like Python lambdas on steroids. One nice feature of arrow functions is that they lexically bind the this variable so we can rewrite the above to read\nvar thing = {\n  value: 1,\n  method: function () {\n    var func = () =&gt; this.value;\n    return func();\n  }\n};\n\nconsole.log(thing.method());\nwhich correctly outputs 1.\n\n\nClasses\nPrior to ES2015, classes in Javascript had to be implemented with a function:\nfunction MyClass(value) {\n  this.value = value;\n}\n\nvar instance = new MyClass(10);\nImplementing inheritance was cumbersome and required the use of the prototype attribute:\nfunction Programmer(language) {\n  this.language = language;\n}\n\n// Add a method to the Programmer prototype\nProgrammer.prototype.programThings = function () {\n  console.log(\"Favorite language: \" + this.language);\n  console.log(this instanceof Programmer);\n};\n\n// Create child classes\nfunction PythonProgrammer() {\n  Programmer.call(this);\n  this.language = \"Python\";\n}\n\nPythonProgrammer.prototype = Object.create(Programmer.prototype);\n\nfunction JavascriptProgrammer() {\n  Programmer.call(this);\n  this.language = \"Javascript\";\n}\n\nJavascriptProgrammer.prototype = Object.create(Programmer.prototype);\n\nvar pythonProgrammer = new PythonProgrammer();\nvar jsProgrammer = new JavascriptProgrammer();\n\npythonProgrammer.programThings();\njsProgrammer.programThings();\n\n/* Output:\n\nFavorite language: Python\ntrue\nFavorite language: Javascript\ntrue */\nWith ES2015, classes can be defined in a more Pythonic way:\nclass MyBaseClass {\n  constructor(value) {\n    this.value = value;\n  }\n\n  method() {\n    return this.value;\n  }\n}\n\nclass MyNewClass extends MyBaseClass {\n  secondMethod() {\n    return this.value + 1;\n  }\n}\n\nvar instance = new MyNewClass(10);\nconsole.log(instance.method());\nconsole.log(instance.secondMethod());\nwhich outputs\n10\n11\n\n\nTemplate strings\nDespite the Zen of Python suggesting that “there should be one– and preferably only one –obvious way to do it,” there are 3 ways to format strings as of Python 3.6. The newest of these ways is the so-called “f-string” introduced by PEP 498. This allows you to have variables dynamically inserted into strings without having to explicitly use %-formatting or the str.format method:\nx = 20\nprint(f'{x} is 20')\nES2015 has a similar concept in template strings:\nvar x = 20;\nconsole.log(`${x} is 20.`);\nLike Python docstrings, Javascript template strings can also span multiple lines:\n`this is\nok\neven if it is\na pointless string.`\n\n\nIterators, generators, and promises\nOne of the most useful features of Python is to be able to easily iterate over a list:\nfor x in y:\n    print(x)\nJavascript has long had, for example, Array.prototype.forEach() to iterate over an array, but this requires the use of a callback function to act on each value in the array. Javascript now has the more Pythonic for ... in and for ... of statements:\nvar list = [1, 2, 3, 4, 5];\nvar obj = {\n  a: 1,\n  b: 2,\n  c: 3\n};\n\n// for ... in on list makes x take on the *indeces* of list\nconsole.log(\"for (x in list)\");\nfor (let x in list) {\n  console.log(x);\n}\n\n// for ... of on list makes x take on the *values* of list\nconsole.log(\"for (x of list)\");\nfor (let x of list) {\n  console.log(x);\n}\n\n// for ... in on obj makes x take on the *keys* of obj\nconsole.log(\"for (x in obj)\");\nfor (let x in obj) {\n  console.log(x);\n}\nGenerators are used frequently in Python to efficiently iterate over values without having to pre-compute and store in memory each loop iteration’s result. In Javascript, a function must explicitly be declared a generator by denoting it a function* and using the familiar yield keyword:\nfunction* itsATrap() {\n  while (true) {\n    yield \"What is it?\";\n    yield \"It's a trap!\";\n  }\n}\n\nvar isItATrap = itsATrap();\nconsole.log(isItATrap.next().value);\nconsole.log(isItATrap.next().value);\nconsole.log(isItATrap.next().value);\nconsole.log(isItATrap.next().value);\n\n\nOther newer features\nThere are quite a few other nice things introduced by ES2015, but the above illustrates features that are perhaps the most welcomed by Python programmers. For a good overview of all newer Javascript features, see this, for example."
  },
  {
    "objectID": "blog/javascript-for-python-programmers.html#pythonic-javascript-cheat-sheet",
    "href": "blog/javascript-for-python-programmers.html#pythonic-javascript-cheat-sheet",
    "title": "Javascript for Python programmers",
    "section": "Pythonic Javascript cheat sheet",
    "text": "Pythonic Javascript cheat sheet\nTo summarize, what follows are a series of short snippets showing some common Pythonic concepts and their Javascript analogs.\n\nException handling\ntry:\n    thing()\nexcept Exception:\n    print(\"oh no!\")\n\nraise ValueError(\"not a good value\")\ntry {\n  thing();\n} catch (error) {\n  console.error(\"oh no!\");\n}\n\nthrow \"not a good value\";\n\n\nIterators\narr = [1, 2, 3]\nobj = {\n    \"a\": 1,\n    \"b\": 2,\n    \"c\": 3\n}\n\nfor val in arr:\n    print(val)\n\nfor key in obj:\n    print(key)\nvar arr = [1, 2, 3];\nvar obj = {\n  a: 1,\n  b: 2,\n  c: 3\n};\n\nfor (let val of arr) {\n  console.log(val);\n}\n\n// or...\narr.forEach((value, index) =&gt; {\n  console.log(value);\n});\n\nfor (let key in obj) {\n  console.log(key);\n}\n\n\nGenerators\ndef gen(x):\n    while True:\n        yield x\n        x = x + 1\nfunction* gen(x) {\n  while (true) {\n    yield x;\n    x++;\n  }\n}\n\n\nClasses\nclass Thing:\n    def __init__(self, a):\n        self.a = a\n\n    def add_one(self):\n        return self.a + 1\n\nclass OtherThing(Thing):\n    def __init__(self, a, b):\n        super(OtherThing, self).__init__(a)\n        self.b = b\n\n    def add_things(self):\n        return self.a + self.b\nclass Thing {\n  constructor(a) {\n    this.a = a;\n  }\n\n  addOne() {\n    return this.a + 1;\n  }\n}\n\nclass OtherThing extends Thing {\n  constructor(a, b) {\n    super(a);\n    this.b = b;\n  }\n\n  addThings() {\n    return this.a + this.b;\n  }\n}\n\n\nFunctional programming\n\nLambdas\nexpression = lambda a, b: a + b\n// Arrow functions are more powerful than Python lambdas, but not in\n// this example!\nlet expression = (a, b) =&gt; a + b;\n\n// or...\nlet sameThing = function (a, b) {\n  return a + b;\n}\n\n\nMapReduce\nfrom functools import reduce\n\nmapped = map(lambda a: a + 1, range(10))\nprint(reduce(lambda a, b: a + b, mapped))\nlet arr = [];\nfor (let i = 0; i &lt; 10; i++) {\n  arr.push(i);\n}\nlet mapped = arr.map((a) =&gt; a + 1);\nconsole.log(arr.reduce((a, b) =&gt; a + b));"
  },
  {
    "objectID": "blog/javascript-for-python-programmers.html#final-thoughts",
    "href": "blog/javascript-for-python-programmers.html#final-thoughts",
    "title": "Javascript for Python programmers",
    "section": "Final thoughts",
    "text": "Final thoughts\nThese days, Javascript the language is much improved and potentially more Pythonic than ever before. Approaching a little Javascript from the perspective of a Python programmer, you can write good, clear code while avoiding many of the (mostly outdated) common pitfalls often brought up by Javascript detractors."
  },
  {
    "objectID": "blog/javascript-for-python-programmers.html#footnotes",
    "href": "blog/javascript-for-python-programmers.html#footnotes",
    "title": "Javascript for Python programmers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot that modern Javascript tooling is really so good at reducing complexity… More on this later.↩︎\nOf course, there are tools like webpack that can let you use modern Javascript modules in the browser, but this requires the step of bundling all the Javascript sources into a browser-friendly bundle. Unless you are willing to dive deep into the, um, interesting world of Javascript tooling, I recommend against this as you get started with more complex Javascript.↩︎\nAlthough it would be a lot nicer if there were actually a standard library to speak of.↩︎"
  },
  {
    "objectID": "blog/art-with-wayland.html",
    "href": "blog/art-with-wayland.html",
    "title": "Using ART with Wayland",
    "section": "",
    "text": "Lately I’ve been experimenting with using ART, a raw photo editing application which is a fork of RawTherapee. Compared to the excellent Darktable, ART has a slightly easier learning curve and can yield decent results pretty quickly. The binaries provided for Linux however do not work on Wayland and the official recommendation is to build binaries yourself. Luckily there is a much easier workaround by setting the GDK_BACKEND environment variable:\nGDK_BACKEND=x11 ./ART"
  },
  {
    "objectID": "blog/til-vscode-options.html",
    "href": "blog/til-vscode-options.html",
    "title": "TIL: Handy VS Code options",
    "section": "",
    "text": "Here are a couple of quick tips that I learned today (well, technically a few days ago).\n\nFixing missing breakpoints\nWhen using VS Code with Python, if tests are ignoring breakpoints it might be because coverage is enabled by default. To disable it, and make breakpoints work again, add the following to .vscode/settings.json:\n{\n  \"python.testing.pytestArgs\": [\"--no-cov\"]\n}\n\n\nCustomizing the title bar\nWith a lot of projects it can be helpful to have a visual indicator to hint at which one is active. You can customize the title bar background color with the following added to settings.json:\n{\n  \"workbench.colorCustomizations\": {\n    \"titleBar.activeBackground\": \"#35257d\"\n  }\n}"
  },
  {
    "objectID": "blog/blocking-code-to-tornado-coroutine.html",
    "href": "blog/blocking-code-to-tornado-coroutine.html",
    "title": "Running (possibly) blocking code like a Tornado coroutine",
    "section": "",
    "text": "One of the main benefits of using the Tornado web server is that it is (normally) a single-threaded, asynchronous framework that can rely on coroutines for concurrency. Many drivers already exist to provide a client library utilizing the Tornado event loop and coroutines (e.g., the Motor MongoDB driver).\nTo write your own coroutine-friendly code for Tornado, there are a few different options available, all requiring that you somehow wrap blocking calls within a Future so as to allow the event loop to continue executing. Here, I demonstrate one recipe to do just this by utilizing Executor objects from the concurrent.futures module. We start with the imports:\nimport random\nimport time\nfrom tornado import gen\nfrom tornado.concurrent import run_on_executor, futures\nfrom tornado.ioloop import IOLoop\nWe will be using the run_on_executor decorator which requires that the class whose methods we decorate have some type of Executor attribute (the default is to use the executor attribute, but a different Executor can be used with a keyword argument passed to the decorator). We’ll create a class to run our asynchronous tasks and give it a ThreadPoolExecutor for executing tasks. In this contrived example, our long running task just sleeps for a random amount of time:\nclass TaskRunner(object):\n    def __init__(self, loop=None):\n        self.executor = futures.ThreadPoolExecutor(4)\n        self.loop = loop or IOLoop.instance()\n\n    @run_on_executor\n    def long_running_task(self):\n        tau = random.randint(0, 3)\n        time.sleep(tau)\n        return tau\nNow, from within a coroutine, we can let the tasks run as if they were normal coroutines:\nloop = IOLoop() # this is necessary if running as an ipynb!\ntasks = TaskRunner(loop)\n\n@gen.coroutine\ndef do_stuff():\n    result = yield tasks.long_running_task()\n    raise gen.Return(result)\n\ndef do_other_stuff():\n    print(random.random())\nFinally, in the main coroutine:\n@gen.coroutine\ndef main():\n    for i in range(10):\n        stuff = yield do_stuff()\n        print(stuff)\n        do_other_stuff()\n\nloop.run_sync(main)\nWhich produces output like:\n3\n0.6012166386789509\n1\n0.9235652108721132\n0\n0.42316507955015026\n3\n0.9766563871068523\n1\n0.21032495467534018\n2\n0.15572313672917715\n0\n0.8767039780374377\n3\n0.6542727048597389\n2\n0.3623342196737247\n0\n0.30042493880819876\nUsing this general pattern, it is rather easy to adapt blocking calls to Tornado’s coroutines. Note that the example code can be found here."
  },
  {
    "objectID": "blog/ipc-with-sqlite.html",
    "href": "blog/ipc-with-sqlite.html",
    "title": "Sharing data between processes with SQLite",
    "section": "",
    "text": "Because of the global interpreter lock in CPython, it is sometimes beneficial to use separate processes to handle different tasks. This can pose a challenge for sharing data: it’s generally best to avoid sharing memory between processes for reasons of safety1. One common approach is to use pipes, queues, or sockets to communicate data from one process to another. This approach works quite well, but it can be a bit cumbersome to get right when there are more than two processes involved and you just need to share a small amount of infrequently changing data (say some configuration settings that are loaded after worker processes have already been spawned). In such cases, using a file that each process can read is a simple solution, but may have problems if reading and writing happen simultaneously. Thankfully, SQLite can handle this situation easily!\nI have created a small module (Permadict) which utilizes SQLite to persist arbitrary (picklable) Python objects to a SQLite database using a dict-like interface. This is not a new idea, but it was fun and simple to utilize only the Python standard library to accomplish this. A basic usage example:\nBecause context managers are great, you can also use permadicts that way:"
  },
  {
    "objectID": "blog/ipc-with-sqlite.html#footnotes",
    "href": "blog/ipc-with-sqlite.html#footnotes",
    "title": "Sharing data between processes with SQLite",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course, Python allows you to share memory among processes by using a manager, but this is not always possible depending on the specific use case.↩︎"
  },
  {
    "objectID": "blog/rpi-usb-ethernet-gateway.html",
    "href": "blog/rpi-usb-ethernet-gateway.html",
    "title": "Raspberry Pi as a USB to Ethernet Gateway",
    "section": "",
    "text": "One of the most convenient ways of communicating with experimental devices (such as oscilloscopes, frequency generators, pulse generators, etc.) is via ethernet. The advantages of this over other forms of communication such as GPIB, RS-232 serial ports, etc., is that, provided the device receives a fixed IP address or some sort of dynamic DNS service is used, it doesn’t matter where it is located and specialty cabling can be kept to a minimum. Luckily, most of these devices, even if they are not equipped with ethernet capability, can be made to work over ethernet with some sort of device server (e.g., there are device servers such as those made by Moxa which can “convert” RS-232 serial port communications to ethernet).\nA lot of modern devices come equipped with a USB port on the back which complies with the USBTMC (USB test and measurement class) specifications. Even fairly inexpensive equipment which lacks an ethernet port are likely to have a USB port for USBTMC communications (e.g., the popular and inexpensive Rigol DS1000D series digital oscilloscopes). There exists a USBTMC Linux kernel module which allows for communication with USBTMC devices via /dev/usbtmcNNN device files. This module, coupled with the versatile socat command, can thus allow for transparent communications over ethernet with a USBTMC device as if it were connected via ethernet itself. The rest of this note describes the process for using a Raspberry Pi as a USBTMC to ethernet adapter."
  },
  {
    "objectID": "blog/rpi-usb-ethernet-gateway.html#introduction",
    "href": "blog/rpi-usb-ethernet-gateway.html#introduction",
    "title": "Raspberry Pi as a USB to Ethernet Gateway",
    "section": "",
    "text": "One of the most convenient ways of communicating with experimental devices (such as oscilloscopes, frequency generators, pulse generators, etc.) is via ethernet. The advantages of this over other forms of communication such as GPIB, RS-232 serial ports, etc., is that, provided the device receives a fixed IP address or some sort of dynamic DNS service is used, it doesn’t matter where it is located and specialty cabling can be kept to a minimum. Luckily, most of these devices, even if they are not equipped with ethernet capability, can be made to work over ethernet with some sort of device server (e.g., there are device servers such as those made by Moxa which can “convert” RS-232 serial port communications to ethernet).\nA lot of modern devices come equipped with a USB port on the back which complies with the USBTMC (USB test and measurement class) specifications. Even fairly inexpensive equipment which lacks an ethernet port are likely to have a USB port for USBTMC communications (e.g., the popular and inexpensive Rigol DS1000D series digital oscilloscopes). There exists a USBTMC Linux kernel module which allows for communication with USBTMC devices via /dev/usbtmcNNN device files. This module, coupled with the versatile socat command, can thus allow for transparent communications over ethernet with a USBTMC device as if it were connected via ethernet itself. The rest of this note describes the process for using a Raspberry Pi as a USBTMC to ethernet adapter."
  },
  {
    "objectID": "blog/rpi-usb-ethernet-gateway.html#compiling-the-rpi-kernel",
    "href": "blog/rpi-usb-ethernet-gateway.html#compiling-the-rpi-kernel",
    "title": "Raspberry Pi as a USB to Ethernet Gateway",
    "section": "Compiling the RPi kernel",
    "text": "Compiling the RPi kernel\nThe RPi’s default kernel does not include USBTMC support as a module or built into the kernel. This requires building from scratch, the full details of which can be found here. The basic idea is to grab the RPi kernel source on a fast computer and cross compile it with the USBTMC kernel module1 (or build into the kernel if you prefer).\nThere are a few caveats and pitfalls, so the following provides the step-by-step approach that worked for me. To get started on a 64-bit Linux machine, make sure you have the 32-bit libraries installed. On Debian and derivatives:\nsudo  dpkg  --add-architecture  i386 # enable multi-arch\nsudo  apt-get  update\nsudo  apt-get  install  ia32-libs\nOnce this is done, the following steps will get things working:\n\nGet the RPi kernel source:\ngit init\ngit clone --depth 1 git://github.com/raspberrypi/linux.git\n\n\nGet the compiler for cross-compiling:\ngit clone git://github.com/raspberrypi/tools.git\n\nCompile the kernel\n\nIn the kernel source directory, do:\nmake mrproper\nNext, copy the default configuration file:\ncp arch/arm/configs/bcmrpi_defconfig .config\nSelect the appropriate compiler to use by defining an environment variable that points to the right place (TODO: put the right thing here):\nexport CCPREFIX=/path/to/your/compiler/binary/prefix-of-binary-\nPre-configure everything and accept the defaults:\nyes \"\" | make oldconfig\nNow we can enable building of the usbtmc kernel module. Run make menuconfig.\nNavigate to Device Drivers &gt; USB support &gt; USB Test and Measurement Class support and make sure it is marked M to build a module. Save the configuration file, then exit. Now build the kernel:\nmake ARCH=arm CROSS_COMPILE=${CCPREFIX} -jN\nwhere N is the number of CPU cores + 1 (e.g., if there are 4 cores, N = 5). This step will take several minutes on a reasonably fast computer. Next build the modules:\nmake ARCH=arm CROSS_COMPILE=${CCPREFIX} modules\n\n\nTransferring the kernel:\nFirst copy to the RPi:\nscp arch/arm/boot/Image pi@yourpi:kernel_someuniqueid.img\nThen on the RPi, copy this over to /boot:\nsudo cp kernel_someuniqueid.img /boot\nEdit the bootloader configuration file to use the new kernel by making sure the following line appears:\nkernel=kernel_someuniqueid.img\nand comment out any other kernel=... lines.\n\n\nTransferring the kernel modules:\nOn the build machine, make a temporary directory to install modules to:\nmkdir ~/modules\nexport MODULES_TEMP=~/modules\nIn the build directory:\nmake ARCH=arm CROSS_COMPILE=${CCPREFIX} INSTALL_MOD_PATH=${MODULES_TEMP} modules_install\nNow in the temporary directory, there should be a lib directory. We don’t need the source/headers, so remove them (otherwise you might run out of space on the RPi SD card!). Transfer these over to the RPi:\nscp -r lib pi@yourpi:\nOn the RPi, copy and overwrite the contents of lib into /lib:\nsudo cp -f lib/* /lib\nOnly do this step while running a different version of the kernel than what you compiled!\n\n\nReboot.\n\n\nLoad the module:\nsudo modprobe usbtmc\n\n\nConnect the USB device.\nThere should now be a device named /dev/usbtmc0."
  },
  {
    "objectID": "blog/rpi-usb-ethernet-gateway.html#talking-to-the-device",
    "href": "blog/rpi-usb-ethernet-gateway.html#talking-to-the-device",
    "title": "Raspberry Pi as a USB to Ethernet Gateway",
    "section": "Talking to the device",
    "text": "Talking to the device\nA Python script for piping data to and from a USBTMC device can be found here. It should be run through socat which does the more difficult work of properly transferring packets. The socat command I use is\nsocat tcp-listen:5025,fork,reuseaddr,crnl,tcpwrap=script\\\n    EXEC:\"python usbtmc_pipe.py\",su-d=pi,pty,echo=0"
  },
  {
    "objectID": "blog/rpi-usb-ethernet-gateway.html#other-notes",
    "href": "blog/rpi-usb-ethernet-gateway.html#other-notes",
    "title": "Raspberry Pi as a USB to Ethernet Gateway",
    "section": "Other notes",
    "text": "Other notes\nIt turns out that it is not necessary to use the kernel module to talk to USBTMC devices. A pure Python implementation of using the USBTMC protocol also exists. This has the advantage of not requiring a custom kernel for the RPi, but it adds the slight complexity of needing to specify vendor and product IDs.\n\nFootnotes"
  },
  {
    "objectID": "blog/rpi-usb-ethernet-gateway.html#footnotes",
    "href": "blog/rpi-usb-ethernet-gateway.html#footnotes",
    "title": "Raspberry Pi as a USB to Ethernet Gateway",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the make menuconfig configuration menus, the option can be found under Device Drivers &gt; USB support &gt; USB Test and   Measurement Class support.↩︎"
  },
  {
    "objectID": "blog/simplifying-argparse.html",
    "href": "blog/simplifying-argparse.html",
    "title": "Simplifying argparse usage with subcommands",
    "section": "",
    "text": "One of the best things about Python is its standard library: it’s frequently possible to create complex applications while requiring few (if any) external dependencies. For example, command line interfaces can be easily built with the argparse module. Despite this, there exist several alternative, third-party modules (e.g., docopt, click, and begins). These all tend to share similar motivations: while argparse is powerful, it is by inherently verbose and is therefore cumbersome to use for more complex CLIs which use advanced features such as subcommands. Nevertheless, I tend to prefer sticking with argparse in part because I am already familiar with the API and because using it means I don’t need to bring in another dependency from PyPI just to add a small bit of extra functionality. The good news is that with a simple decorator and a convenience function, writing CLIs with subcommands in argparse is pretty trivial and clean.\nStart by creating a parser and subparsers in cli.py:\nfrom argparse import ArgumentParser\n\ncli = ArgumentParser()\nsubparsers = cli.add_subparsers(dest=\"subcommand\")\nNote that we are storing the name of the called subcommand so that we can later print help if either no subcommand is given or if an unrecognized one is. Now we can define a decorator to turn a function into a subcommand:\ndef subcommand(args=[], parent=subparsers):\n    def decorator(func):\n        parser = parent.add_parser(func.__name__, description=func.__doc__)\n        for arg in args:\n            parser.add_argument(*arg[0], **arg[1])\n        parser.set_defaults(func=func)\n    return decorator\nWhat this does is take the wrapped function and use its name and docstring for the subcommand name and help string, respectively. Next it automatically adds arguments for the subcommand from a list passed to the decorator. In order to dispatch the command later, the usual parser.set_defaults method is used to store the function itself in the func variable.\nIn the simplest case, we can create a subcommand which requires no arguments as follows:\n@subcommand()\ndef nothing(args):\n    print(\"Nothing special!\")\nMeanwhile, in our main function, we dispatch the subcommand as follows:\nif __name__ == \"__main__\":\n    args = cli.parse_args()\n    if args.subcommand is None:\n        cli.print_help()\n    else:\n        args.func(args)\nNow running python cli.py nothing will run the nothing function and simply print Nothing special! to stdout.\nMore often, subcommands require their own set of options. In the definition of the subcommand decorator above, these options can be given as a list of length-2 lists that contain the name or flags for the argument and all keyword arguments used by ArgumentParser.add_argument. This is a bit cumbersome as is, so it’s useful to define a small helper function that takes arguments just like ArgumentParser.add_argument:\ndef argument(*name_or_flags, **kwargs):\n    return ([*name_or_flags], kwargs)\nNow we can define commands with arguments like so:\n@subcommand([argument(\"-d\", help=\"Debug mode\", action=\"store_true\")])\ndef test(args):\n    print(args)\n\n\n@subcommand([argument(\"-f\", \"--filename\", help=\"A thing with a filename\")])\ndef filename(args):\n    print(args.filename)\n\n\n@subcommand([argument(\"name\", help=\"Name\")])\ndef name(args):\n    print(args.name)\nThat’s all there is to it! Quite a bit better than the default way to build a CLI with subcommands. The full example can be found here."
  },
  {
    "objectID": "blog/flask-websockets-with-tornado.html",
    "href": "blog/flask-websockets-with-tornado.html",
    "title": "Using websockets with Flask via Tornado",
    "section": "",
    "text": "I’ve been working on some projects for the lab that involve remotely controlling hardware to perform various tasks. Since the hardware in question is shared between different experiments, some sort of asynchronous solution is needed, and a web-based client coupled with websockets seemed to be the best bet (this also leaves the option open in the future to write a standalone client that is not browser-based if desired).\nThere is no shortage of web frameworks for Python. Some of the more popular ones are Django, Flask, Tornado, and Pyramid. Of these, I greatly prefer Flask for a number of reasons:\nThis is not to say that the other options are bad, but having looked at all of them, Flask suits me best. The one problem: only Tornado directly supports websockets since it is both an HTTP server and a web framework in one, whereas the others utilize WSGI for deployment.\nLuckily, it is possible to leverage both the excellent asynchronous features of Tornado and the power and ease of use of Flask through Tornado’s ability to serve WSGI apps with tornado.wsgi.WSGIContainer. The Flask documentation shows a very simple example on how to do just that.\nIntegrating websockets into a Flask app is now pretty easy. Here’s an example on the server side:\nThe client-side Javascript is simple as well:\nThe full demo example can be found here."
  },
  {
    "objectID": "blog/flask-websockets-with-tornado.html#additional-notes",
    "href": "blog/flask-websockets-with-tornado.html#additional-notes",
    "title": "Using websockets with Flask via Tornado",
    "section": "Additional notes",
    "text": "Additional notes\nThere already exist at least two extensions for Flask to use websockets:\n\nFlask-Sockets\nFlask-SocketIO\n\nHowever, both of these are based on gevent. While gevent is nice, it still has limited Python 3 support and does not work on Windows (sadly, a requirement for some hardware drivers)."
  },
  {
    "objectID": "blog/hg-import-from-another-repo.html",
    "href": "blog/hg-import-from-another-repo.html",
    "title": "Importing one Mercurial repository into another",
    "section": "",
    "text": "In the ion trap group, we usually use Mercurial for version controlling software we write for experimental control, data analysis, and so on. This post outlines how to import the full history of one repository into another. This can be useful for cases where it makes sense to move a sub-project directly into its parent, for example."
  },
  {
    "objectID": "blog/hg-import-from-another-repo.html#convert-the-soon-to-be-child-repository",
    "href": "blog/hg-import-from-another-repo.html#convert-the-soon-to-be-child-repository",
    "title": "Importing one Mercurial repository into another",
    "section": "Convert the soon-to-be child repository",
    "text": "Convert the soon-to-be child repository\nWith the Mercurial convert extension, you can rename branches, move, and filter files. As an example, say we have a repo with only the default branch which is to be imported into a super-repository.\nFor starters, we will want all our files in the child repo to be in a subdirectory of the parent repo and not include the child’s .hgignore. To do this, create a file filemap.txt with the following contents:\nrename . child\nexclude .hgignore\nThe first line will move all files in the repo’s top level into a directory named child.\nNext, optionally create a branchmap.txt file for renaming the default branch to something else:\ndefault child-repo\nNow convert:\nhg convert --filemap branchmap.txt --branchmap branchmap.txt child/ converted/"
  },
  {
    "objectID": "blog/hg-import-from-another-repo.html#pull-in-the-converted-repository",
    "href": "blog/hg-import-from-another-repo.html#pull-in-the-converted-repository",
    "title": "Importing one Mercurial repository into another",
    "section": "Pull in the converted repository",
    "text": "Pull in the converted repository\nFrom the parent repo:\nhg pull -f ../converted\nEnsure the child commits are in the draft phase with:\nhg phase -f --draft -r &lt;first&gt;:&lt;last&gt;"
  },
  {
    "objectID": "blog/hg-import-from-another-repo.html#rebase-as-appropriate",
    "href": "blog/hg-import-from-another-repo.html#rebase-as-appropriate",
    "title": "Importing one Mercurial repository into another",
    "section": "Rebase as appropriate",
    "text": "Rebase as appropriate\nhg rebase -s &lt;child rev&gt; -d &lt;parent rev&gt;\nTo keep the child’s changed branch name, use the --keepbranches option."
  },
  {
    "objectID": "blog/hg-import-from-another-repo.html#references",
    "href": "blog/hg-import-from-another-repo.html#references",
    "title": "Importing one Mercurial repository into another",
    "section": "References",
    "text": "References\n\nhttps://mercurial.selenic.com/wiki/ConvertExtension\nhttps://mercurial.selenic.com/wiki/Phases\nhttps://mercurial.selenic.com/wiki/RebaseExtension\nhttps://stackoverflow.com/questions/3214717/how-can-i-import-a-mercurial-repo-including-history-into-another-mercurial-rep\nhttps://stackoverflow.com/questions/3338672/mercurial-convert-clones-to-branches"
  },
  {
    "objectID": "blog/conda-kernels.html",
    "href": "blog/conda-kernels.html",
    "title": "Jupyter integration with conda environments",
    "section": "",
    "text": "How to create new Jupyter notebooks with different conda environments:\n\nIn the base environment, conda install -y nb_conda\nIn every environment you plan to use, conda install -y ipykernel"
  },
  {
    "objectID": "blog/matplotlib-color-cycle.html",
    "href": "blog/matplotlib-color-cycle.html",
    "title": "Getting Matplotlib’s colors in order",
    "section": "",
    "text": "Matplotlib can be very easy to use at times, especially if you just want to make a simple “y vs. x” type of plot. But when it comes to specialized customization, it can be a bit challenging to find the proper solution. The situation is not helped by the fact that a lot of times, an obscure answer on Stack Overflow no longer works because the API changed.\nOne common need is to color things in the same way. For example, say you want to plot two dependent variables with widely different scales that share an independent variable. This is often represented by having two separate vertical axes which are colored to match the lines or markers of each data set. The most basic approach is to manually assign colors for the lines and axes, but if using a custom style, such as the ggplot style, we need a way to access the color cycle used if we want to remain consistent with the selected style. Here is the best way I have found which works at least in version 1.5:\ncolors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolors should now be a list which contains the colors defined in order by the current style."
  },
  {
    "objectID": "blog/tornado-background-tasks.html",
    "href": "blog/tornado-background-tasks.html",
    "title": "Background tasks with Tornado",
    "section": "",
    "text": "I have been using Tornado lately for distributed control of devices in the lab where an asynchronous framework is advantageous. In particular, we have a HighFinesse wavelength meter which we use to monitor and stabilize several lasers (up to 14 at a time). Previously, a custom server for controlling this wavemeter was written using Twisted, but that has proven difficult to upgrade, distribute, and maintain.\nOne thing that is common for such a control scenario is that data needs to be refreshed continuously while still allowing incoming connections from clients and appropriately executing remote procedure calls. One method would be to periodically interrupt the Tornado IO loop to refresh data (and in fact, Tornado has a class to make this easy for you in tornado.ioloop.PeriodicCallback). This can be fine if the data refreshing does not take too much time, but all other operations will be blocked until the callback is finished, which can be a problem if the refreshing operation is slow. Another option is to have an additional thread separate from the Tornado IO loop that handles refreshing data. This certainly works, but adds the complexity of needing to use thread-safe communications to stop the thread when the main application is shut down or when other tasks depend on the successful completion of the refresh.\nLuckily, Tornado also includes a decorator, tornado.concurrent.run_on_executor, to run things in the background for you using Python’s concurrent.futures module (which is standard starting in Python 3.3 and backported for other versions). Then instead of writing the refresh function as a loop that runs in the background, you instead have its final call be to add itself back as a callback on the IO loop. This makes shutdown trivial since only the IO loop needs to be stopped when the program is closed. A refresh function could thus look something like this:\n@tornado.concurrent.run_on_executor\ndef refresh():\n    do_something_that_takes_a_while()\n    tornado.ioloop.IOLoop.instance().add_callback(self.refresh)\nNow after refresh is called once, it will continuously run until the IO loop is stopped.\nFor a more complete example, I have written a small demo."
  },
  {
    "objectID": "blog/sqlalchemy-timestamps.html",
    "href": "blog/sqlalchemy-timestamps.html",
    "title": "Ensuring timestamp storage in UTC with SQLAlchemy",
    "section": "",
    "text": "Naively one might think that using defining a column with DateTime(timezone=True) when defining a SQL table with SQLAlchemy would result in a timezone-aware datetime object when loading into Python from the database. This doesn’t always work however, in particular when using SQLite. Note the following behavior:\nfrom datetime import datetime, timezone\nimport sqlalchemy as sa\n\nengine = sa.create_engine(\"sqlite:///\")\nmetadata = sa.MetaData()\nbad_datetimes = sa.Table(\n    \"bad_datetimes\", metadata,\n    sa.Column(\"datetime\", sa.DateTime(timezone=True))\n)\nmetadata.create_all(bind=engine)\n\n# Try inserting both a naive datetime and a timezone-aware datetime\nengine.execute(bad_datetimes.insert().values([\n    {\"datetime\": datetime.now()},\n    {\"datetime\": datetime.now(timezone.utc)}\n]))\n\nprint(engine.execute(bad_datetimes.select()).fetchall())\n\n# Results in:\n# [(datetime.datetime(2019, 3, 29, 13, 56, 1, 224546),), (datetime.datetime(2019, 3, 29, 19, 56, 1, 224554),)]\nSo despite telling DateTime that we want timezones, that information has been lost!\nTo resolve this behavior, we can use sa.types.TypeDecorator to always get timezone-aware datetimes:\nclass TimeStamp(sa.types.TypeDecorator):\n    impl = sa.types.DateTime\n    LOCAL_TIMEZONE = datetime.utcnow().astimezone().tzinfo\n\n    def process_bind_param(self, value: datetime, dialect):\n        if value.tzinfo is None:\n            value = value.astimezone(self.LOCAL_TIMEZONE)\n\n        return value.astimezone(timezone.utc)\n\n    def process_result_value(self, value, dialect):\n        if value.tzinfo is None:\n            return value.replace(tzinfo=timezone.utc)\n\n        return value.astimezone(timezone.utc)\n\ngood_datetimes = sa.Table(\n    \"good_datetimes\", metadata,\n    sa.Column(\"datetime\", TimeStamp())\n)\nmetadata.create_all(bind=engine)\n\nengine.execute(good_datetimes.insert().values([\n    {\"datetime\": datetime.now()},\n    {\"datetime\": datetime.now(timezone.utc)}\n]))\nprint(engine.execute(good_datetimes.select()).fetchall())\n\n# Results in:\n# [(datetime.datetime(2019, 3, 29, 20, 1, 10, 718427, tzinfo=datetime.timezone.utc),),\n#  (datetime.datetime(2019, 3, 29, 20, 1, 10, 718431, tzinfo=datetime.timezone.utc),)]\nNote that in this example we’re assuming that naive datetimes are always in the local timezone. This may not always be the right assumption, in which case we would probably want to just enforce the usage of timezone-aware datetimes in the first place."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a scientific software developer, data engineer, and physicist. I currently work as a Senior Software Engineer at Ascend Analytics in Boulder, Colorado. In a past life I was an experimental atomic physicist in the ion trap group after earning my PhD at Georgia Tech.\nIn my spare time, I enjoy photography, various outdoor activities, reading, and working on various projects related to Arduino, Raspberry Pi, and open source software. Lately I have been particularly interested in interacting with physical devices via web-based interfaces. Most of my projects can be found on GitHub (see also my Gists).\nFor fun, and as a service to the ion trapping community, I maintain the Ion Trapping Periodic Table. This aims to be a handy reference for ion trappers with information on transition wavelengths, photoionization schemes, and more."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\n\nEmail: mike at (my last name in lower case) dot net\nLinkedIn: michael-v-depalatis\nGitHub: mivade"
  },
  {
    "objectID": "index.html#friends",
    "href": "index.html#friends",
    "title": "About",
    "section": "Friends",
    "text": "Friends\nThis is not an exhaustive list. It only lists people who have web sites that I know about.\n\nRichard Darst\nJohn Dowdle\nAna Guerrero"
  }
]