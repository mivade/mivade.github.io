{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"About I am a scientific software developer in the Computational Memory Lab at the University of Pennsylvania where I am a part of the DARPA RAM team. Previously, I was an experimental atomic physicist. I was a postdoc in the ion trap group of Prof. Michael Drewsen at Aarhus University and earned my PhD in Mike Chapman's lab at Georgia Tech . In my spare time, I enjoy photography, various outdoor activities, reading, and working on various projects related to Arduino , Raspberry Pi , and open source software. Lately I have been particularly interested in interacting with physical devices via web-based interfaces. Most of my projects can be found on GitHub (see also my Gists ). For fun, and as a service to the ion trapping community, I maintain the Ion Trapping Periodic Table . This aims to be a handy reference for ion trappers with information on transition wavelengths, photoionization schemes, and more. Contact Email : mike at (my last name in lower case) dot net GPG : 9C6E 918F 885D 19AA EC7E FB76 25A6 CDBF C58D 27A5 Quick links Debian DebianScience Emacs More here . Friends This is not an exhaustive list. It only lists people who have web sites that I know about. Richard Darst John Dowdle Ana Guerrero Ryan Reece","title":"About"},{"location":"index.html#about","text":"I am a scientific software developer in the Computational Memory Lab at the University of Pennsylvania where I am a part of the DARPA RAM team. Previously, I was an experimental atomic physicist. I was a postdoc in the ion trap group of Prof. Michael Drewsen at Aarhus University and earned my PhD in Mike Chapman's lab at Georgia Tech . In my spare time, I enjoy photography, various outdoor activities, reading, and working on various projects related to Arduino , Raspberry Pi , and open source software. Lately I have been particularly interested in interacting with physical devices via web-based interfaces. Most of my projects can be found on GitHub (see also my Gists ). For fun, and as a service to the ion trapping community, I maintain the Ion Trapping Periodic Table . This aims to be a handy reference for ion trappers with information on transition wavelengths, photoionization schemes, and more.","title":"About"},{"location":"index.html#contact","text":"Email : mike at (my last name in lower case) dot net GPG : 9C6E 918F 885D 19AA EC7E FB76 25A6 CDBF C58D 27A5","title":"Contact"},{"location":"index.html#quick-links","text":"Debian DebianScience Emacs More here .","title":"Quick links"},{"location":"index.html#friends","text":"This is not an exhaustive list. It only lists people who have web sites that I know about. Richard Darst John Dowdle Ana Guerrero Ryan Reece","title":"Friends"},{"location":"archives.html","text":"Archives 2018-05-10 Jupyter integration with conda environments 2018-10-24 Useful BASH shortcuts for conda 2018-01-15 Multiline lambdas 2017-03-19 Sharing data between processes with SQLite 2016-07-03 Fitting with lmfit 2016-07-21 Getting Matplotlib's colors in order 2016-06-25 Using Postgres as a time series database 2016-10-12 Simplifying argparse usage with subcommands 2016-10-10 Javascript for Python programmers 2015-08-03 Running (possibly) blocking code like a Tornado coroutine 2015-02-28 Background tasks with Tornado 2015-02-14 Flask and server-sent events 2015-11-04 Importing one Mercurial repository into another 2014-03-08 Raspberry Pi as a USB to Ethernet Gateway 2014-12-07 ssh-agent for sudo authentication with a passwordless account 2014-12-15 Using websockets with Flask via Tornado","title":"Archives"},{"location":"archives.html#archives","text":"2018-05-10 Jupyter integration with conda environments 2018-10-24 Useful BASH shortcuts for conda 2018-01-15 Multiline lambdas 2017-03-19 Sharing data between processes with SQLite 2016-07-03 Fitting with lmfit 2016-07-21 Getting Matplotlib's colors in order 2016-06-25 Using Postgres as a time series database 2016-10-12 Simplifying argparse usage with subcommands 2016-10-10 Javascript for Python programmers 2015-08-03 Running (possibly) blocking code like a Tornado coroutine 2015-02-28 Background tasks with Tornado 2015-02-14 Flask and server-sent events 2015-11-04 Importing one Mercurial repository into another 2014-03-08 Raspberry Pi as a USB to Ethernet Gateway 2014-12-07 ssh-agent for sudo authentication with a passwordless account 2014-12-15 Using websockets with Flask via Tornado","title":"Archives"},{"location":"resources.html","text":"Resources Physics and Math MathWorld - An excellent resource for looking up math concepts. Eric Weisstein's World of Physics - Similar to MathWorld, only with physics. HyperPhysics - Another great (though more elementary) physics resource. Maxima - An excellent GPL'd computer algebra system. I'd recommend SymPy instead these days, however. Falstad Math and Physics Applets - Some excellent applets on math, quantum mechanics, electrodynamics, etc. Electromagnetic Waves and Antennas - A freely available book (with inidividual chapters in PDF form). Encyclopedia of Laser Physics and Technology Python Numpy Scipy Matplotlib Tornado reStructuredText I previously used reST for notes and as a plaintext format. I've now switched over mostly to Markdown , but since reST is used heavily for Pythonic documentation, it's still good to know. A ReStructuredText Primer Quick reStructuredText Directives reStructuredText tips and tricks LaTeX Getting to grips with Latex - An excellent introduction to LaTeX. Online Tutorials for LaTeX - Some more LaTeX tutorials. LaTeX Beamer - A LaTeX style class for creating presentations. Check out the themes gallery . LaTeX Letter Format - For those times when you need to actually send a letter, this is how you do it with LaTeX. James Bray's Latex Help - Various useful tips with selected topics in LaTeX. Linux Debian - The best Linux distribution out there. autotut - A guide to using the GNU autotools (autoconf, automake, autoheader). Gnuplot Not So Frequently Asked Questions - An incredible overview of gnuplot. Advanced Bash-Scripting Guide - Everything you ever needed to know about Bash scripting. Filesystem Hierarchy Standard - Information what everything in / on your Linux system actually means. Electronics Dangerous Prototypes - Many interesting open source hardware projects dirtypcbs.com - Cheap PCB service (China) Seeed Studio - Electronics components and cheap PCB fabrication (China) KiCAD Libraries and modules kicadlib.org KiCAD libraries converted from Eagle HOWTOs Making New Components and Modules (Footprints) in KiCAD Arduino Ethernet DIY link 1 DIY link 2","title":"Resources"},{"location":"resources.html#resources","text":"","title":"Resources"},{"location":"resources.html#physics-and-math","text":"MathWorld - An excellent resource for looking up math concepts. Eric Weisstein's World of Physics - Similar to MathWorld, only with physics. HyperPhysics - Another great (though more elementary) physics resource. Maxima - An excellent GPL'd computer algebra system. I'd recommend SymPy instead these days, however. Falstad Math and Physics Applets - Some excellent applets on math, quantum mechanics, electrodynamics, etc. Electromagnetic Waves and Antennas - A freely available book (with inidividual chapters in PDF form). Encyclopedia of Laser Physics and Technology","title":"Physics and Math"},{"location":"resources.html#python","text":"Numpy Scipy Matplotlib Tornado","title":"Python"},{"location":"resources.html#restructuredtext","text":"I previously used reST for notes and as a plaintext format. I've now switched over mostly to Markdown , but since reST is used heavily for Pythonic documentation, it's still good to know. A ReStructuredText Primer Quick reStructuredText Directives reStructuredText tips and tricks","title":"reStructuredText"},{"location":"resources.html#latex","text":"Getting to grips with Latex - An excellent introduction to LaTeX. Online Tutorials for LaTeX - Some more LaTeX tutorials. LaTeX Beamer - A LaTeX style class for creating presentations. Check out the themes gallery . LaTeX Letter Format - For those times when you need to actually send a letter, this is how you do it with LaTeX. James Bray's Latex Help - Various useful tips with selected topics in LaTeX.","title":"LaTeX"},{"location":"resources.html#linux","text":"Debian - The best Linux distribution out there. autotut - A guide to using the GNU autotools (autoconf, automake, autoheader). Gnuplot Not So Frequently Asked Questions - An incredible overview of gnuplot. Advanced Bash-Scripting Guide - Everything you ever needed to know about Bash scripting. Filesystem Hierarchy Standard - Information what everything in / on your Linux system actually means.","title":"Linux"},{"location":"resources.html#electronics","text":"Dangerous Prototypes - Many interesting open source hardware projects dirtypcbs.com - Cheap PCB service (China) Seeed Studio - Electronics components and cheap PCB fabrication (China)","title":"Electronics"},{"location":"resources.html#kicad","text":"","title":"KiCAD"},{"location":"resources.html#libraries-and-modules","text":"kicadlib.org KiCAD libraries converted from Eagle","title":"Libraries and modules"},{"location":"resources.html#howtos","text":"Making New Components and Modules (Footprints) in KiCAD","title":"HOWTOs"},{"location":"resources.html#arduino","text":"Ethernet DIY link 1 DIY link 2","title":"Arduino"},{"location":"blog/blocking-code-to-tornado-coroutine.html","text":"One of the main benefits of using the Tornado web server is that it is (normally) a single-threaded, asynchronous framework that can rely on coroutines for concurrency. Many drivers already exist to provide a client library utilizing the Tornado event loop and coroutines (e.g., the Motor MongoDB driver). To write your own coroutine-friendly code for Tornado, there are a few different options available, all requiring that you somehow wrap blocking calls within a Future so as to allow the event loop to continue executing. Here, I demonstrate one recipe to do just this by utilizing Executor objects from the concurrent.futures module. We start with the imports: import random import time from tornado import gen from tornado.concurrent import run_on_executor, futures from tornado.ioloop import IOLoop We will be using the run_on_executor decorator which requires that the class whose methods we decorate have some type of Executor attribute (the default is to use the executor attribute, but a different Executor can be used with a keyword argument passed to the decorator). We'll create a class to run our asynchronous tasks and give it a ThreadPoolExecutor for executing tasks. In this contrived example, our long running task just sleeps for a random amount of time: class TaskRunner(object): def __init__(self, loop=None): self.executor = futures.ThreadPoolExecutor(4) self.loop = loop or IOLoop.instance() @run_on_executor def long_running_task(self): tau = random.randint(0, 3) time.sleep(tau) return tau Now, from within a coroutine, we can let the tasks run as if they were normal coroutines: loop = IOLoop() # this is necessary if running as an ipynb! tasks = TaskRunner(loop) @gen.coroutine def do_stuff(): result = yield tasks.long_running_task() raise gen.Return(result) def do_other_stuff(): print(random.random()) Finally, in the main coroutine: @gen.coroutine def main(): for i in range(10): stuff = yield do_stuff() print(stuff) do_other_stuff() loop.run_sync(main) Which produces output like: 3 0.6012166386789509 1 0.9235652108721132 0 0.42316507955015026 3 0.9766563871068523 1 0.21032495467534018 2 0.15572313672917715 0 0.8767039780374377 3 0.6542727048597389 2 0.3623342196737247 0 0.30042493880819876 Using this general pattern, it is rather easy to adapt blocking calls to Tornado's coroutines. Note that the example code can be found here .","title":"Running (possibly) blocking code like a Tornado coroutine"},{"location":"blog/conda-kernels.html","text":"How to create new Jupyter notebooks with different conda environments: In the base environment, conda install -y nb_conda In every environment you plan to use, conda install -y ipykernel","title":"Jupyter integration with conda environments"},{"location":"blog/conda-shortcuts.html","text":"Anaconda's conda has become the de facto environment management tool in Python-oriented scientific computing. Its lightweight environments are suitable for development and, at least in some cases, deployment. In general, for each new project, I like to create a new environment. Creating a new, mostly empty environment is done with the following command: $ conda env create -y -n my-project-name Then the new environment must be activated with $ conda activate my-project-name If I later want to remove the environment: $ conda env remove -y -n my-project-name This can get tiresome, so I have the following functions defined in my .bashrc : function conact() { conda activate $(basename $(pwd)) } function cenv() { if [ -f environment.yaml ]; then conda env create --file=environment.yaml -n $(basename $(pwd)) else conda create -yn $(basename $(pwd)) fi } function rmcenv() { conda remove -n $(basename $(pwd)) --all } Now when starting a new project, I need only type $ cenv $ conact","title":"Useful BASH shortcuts for conda"},{"location":"blog/flask-sse-demo.html","text":"I recently discovered the existence of the HTML5 server-sent events standard. Although it lacks the bidirectional communications of a websocket, SSE is perfect for the publish-subscribe networking pattern. This pattern just so happens to fit in conveniently with writing software to remotely monitor hardware that many people might want to check in on at the same time. In order to try SSE out within a Flask framework, I put together a simple demo app using gevent . The core of the demo on the Python side looks like this: app = Flask(__name__) def event(): while True: yield 'data: ' + json.dumps(random.rand(1000).tolist()) + '\\n\\n' gevent.sleep(0.2) @app.route('/') def index(): return render_template('index.html') @app.route('/stream/', methods=['GET', 'POST']) def stream(): return Response(event(), mimetype=\"text/event-stream\") This can be run either using gevent's WSGI server or gunicorn using gevent workers. Update 2016-04-21 : There is now a very nice Flask extension called Flask-SSE which handles all of this for you. It additionally supports the concept of channels in order to fine tune what notifications a given client receives.","title":"Flask and server-sent events"},{"location":"blog/flask-websockets-with-tornado.html","text":"I've been working on some projects for the lab that involve remotely controlling hardware to perform various tasks. Since the hardware in question is shared between different experiments, some sort of asynchronous solution is needed, and a web-based client coupled with websockets seemed to be the best bet (this also leaves the option open in the future to write a standalone client that is not browser-based if desired). There is no shortage of web frameworks for Python. Some of the more popular ones are Django , Flask , Tornado , and Pyramid . Of these, I greatly prefer Flask for a number of reasons: Very thorough and easy to read documentation, including \"snippets\" with helpful tips and a very helpful community. Extreme ease of use for both small and large projects. Great use of decorators to further ease development. A large number of extensions to build up a complex project without requiring overhead for simple projects. This is not to say that the other options are bad, but having looked at all of them, Flask suits me best. The one problem: only Tornado directly supports websockets since it is both an HTTP server and a web framework in one, whereas the others utilize WSGI for deployment. Luckily, it is possible to leverage both the excellent asynchronous features of Tornado and the power and ease of use of Flask through Tornado's ability to serve WSGI apps with tornado.wsgi.WSGIContainer . The Flask documentation shows a very simple example on how to do just that. Integrating websockets into a Flask app is now pretty easy. Here's an example on the server side: from __future__ import print_function from flask import Flask, render_template from tornado.wsgi import WSGIContainer from tornado.web import Application, FallbackHandler from tornado.websocket import WebSocketHandler from tornado.ioloop import IOLoop class WebSocket(WebSocketHandler): def open(self): print(\"Socket opened.\") def on_message(self, message): self.write_message(\"Received: \" + message) print(\"Received message: \" + message) def on_close(self): print(\"Socket closed.\") app = Flask('flasknado') @app.route('/') def index(): return render_template('index.html') if __name__ == \"__main__\": container = WSGIContainer(app) server = Application([ (r'/websocket/', WebSocket), (r'.*', FallbackHandler, dict(fallback=container)) ]) server.listen(8080) IOLoop.instance().start() The client-side Javascript is simple as well: var socket = null; $(document).ready(function() { socket = new WebSocket(\"ws://\" + document.domain + \":8080/websocket/\"); socket.onopen = function() { socket.send(\"Joined\"); } socket.onmessage = function(message) { var txt = message.data; $(\".container\").append(\"<p>\" + txt + \"</p>\"); } }); function submit() { var text = $(\"input#message\").val(); socket.send(text); $(\"input#message\").val(''); } The full demo example can be found here . Additional notes There already exist at least two extensions for Flask to use websockets: Flask-Sockets Flask-SocketIO However, both of these are based on gevent . While gevent is nice, it still has limited Python 3 support and does not work on Windows (sadly, a requirement for some hardware drivers).","title":"Using websockets with Flask via Tornado"},{"location":"blog/flask-websockets-with-tornado.html#additional-notes","text":"There already exist at least two extensions for Flask to use websockets: Flask-Sockets Flask-SocketIO However, both of these are based on gevent . While gevent is nice, it still has limited Python 3 support and does not work on Windows (sadly, a requirement for some hardware drivers).","title":"Additional notes"},{"location":"blog/hg-import-from-another-repo.html","text":"In the ion trap group , we usually use Mercurial for version controlling software we write for experimental control, data analysis, and so on. This post outlines how to import the full history of one repository into another. This can be useful for cases where it makes sense to move a sub-project directly into its parent, for example. Convert the soon-to-be child repository With the Mercurial convert extension, you can rename branches, move, and filter files. As an example, say we have a repo with only the default branch which is to be imported into a super-repository. For starters, we will want all our files in the child repo to be in a subdirectory of the parent repo and not include the child's .hgignore . To do this, create a file filemap.txt with the following contents: rename . child exclude .hgignore The first line will move all files in the repo's top level into a directory named child . Next, optionally create a branchmap.txt file for renaming the default branch to something else: default child-repo Now convert: hg convert --filemap branchmap.txt --branchmap branchmap.txt child/ converted/ Pull in the converted repository From the parent repo: hg pull -f ../converted Ensure the child commits are in the draft phase with: hg phase -f --draft -r <first>:<last> Rebase as appropriate hg rebase -s <child rev> -d <parent rev> To keep the child's changed branch name, use the --keepbranches option. References https://mercurial.selenic.com/wiki/ConvertExtension https://mercurial.selenic.com/wiki/Phases https://mercurial.selenic.com/wiki/RebaseExtension https://stackoverflow.com/questions/3214717/how-can-i-import-a-mercurial-repo-including-history-into-another-mercurial-rep https://stackoverflow.com/questions/3338672/mercurial-convert-clones-to-branches","title":"Importing one Mercurial repository into another"},{"location":"blog/hg-import-from-another-repo.html#convert-the-soon-to-be-child-repository","text":"With the Mercurial convert extension, you can rename branches, move, and filter files. As an example, say we have a repo with only the default branch which is to be imported into a super-repository. For starters, we will want all our files in the child repo to be in a subdirectory of the parent repo and not include the child's .hgignore . To do this, create a file filemap.txt with the following contents: rename . child exclude .hgignore The first line will move all files in the repo's top level into a directory named child . Next, optionally create a branchmap.txt file for renaming the default branch to something else: default child-repo Now convert: hg convert --filemap branchmap.txt --branchmap branchmap.txt child/ converted/","title":"Convert the soon-to-be child repository"},{"location":"blog/hg-import-from-another-repo.html#pull-in-the-converted-repository","text":"From the parent repo: hg pull -f ../converted Ensure the child commits are in the draft phase with: hg phase -f --draft -r <first>:<last>","title":"Pull in the converted repository"},{"location":"blog/hg-import-from-another-repo.html#rebase-as-appropriate","text":"hg rebase -s <child rev> -d <parent rev> To keep the child's changed branch name, use the --keepbranches option.","title":"Rebase as appropriate"},{"location":"blog/hg-import-from-another-repo.html#references","text":"https://mercurial.selenic.com/wiki/ConvertExtension https://mercurial.selenic.com/wiki/Phases https://mercurial.selenic.com/wiki/RebaseExtension https://stackoverflow.com/questions/3214717/how-can-i-import-a-mercurial-repo-including-history-into-another-mercurial-rep https://stackoverflow.com/questions/3338672/mercurial-convert-clones-to-branches","title":"References"},{"location":"blog/ipc-with-sqlite.html","text":"Because of the global interpreter lock in CPython, it is sometimes beneficial to use separate processes to handle different tasks. This can pose a challenge for sharing data: it's generally best to avoid sharing memory between processes for reasons of safety[^1]. One common approach is to use pipes, queues, or sockets to communicate data from one process to another. This approach works quite well, but it can be a bit cumbersome to get right when there are more than two processes involved and you just need to share a small amount of infrequently changing data (say some configuration settings that are loaded after worker processes have already been spawned). In such cases, using a file that each process can read is a simple solution, but may have problems if reading and writing happen simultaneously. Thankfully, SQLite can handle this situation easily! I have created a small module ( Permadict ) which utilizes SQLite to persist arbitrary (picklable) Python objects to a SQLite database using a dict-like interface. This is not a new idea , but it was fun and simple to utilize only the Python standard library to accomplish this. A basic usage example: >>> from permadict import Permadict >>> d = Permadict(\"db.sqlite\") >>> d[\"key\"] = \"value\" >>> print(d[\"key\"]) value Because context managers are great, you can also use permadicts that way: >>> with Permadict(\"db.sqlite\") as d: ... d[\"something\"] = 1.2345 ... >>> with Permadict(\"db.sqlite\") as d: ... print(d[\"something\"]) ... 1.2345 [^1]: Of course, Python allows you to share memory among processes by using a manager , but this is not always possible depending on the specific use case.","title":"Sharing data between processes with SQLite"},{"location":"blog/javascript-for-python-programmers.html","text":"Unless you're just writing a simple HTTP API server, any amount of web programming in Python will likely require at least a little bit of Javascript. Like it or not (and I will try to argue in this post that you should like it for what it's good at), Javascript is really the only game in town when it comes to client-side scripting on a web page. Sure, there are a number of Python-to-Javascript transpilers out there , but using these just tends to limit the ability to use new Javascript features as they are rolled out to browsers and may limit the ability to use third-party Javascript libraries. At the very least, using one these transpilers introduces added complexity to deploying a web app[^1]. In this post, I will describe some things I've learned about Javascript from the perspective of someone who prefers to use Python as much as possible. This guide is mainly aimed at scientists and others who are not primarily programmers but who may find it useful to make a web app for their main work. It is assumed that the reader is at least moderately familiar with Javascript (Mozilla has a nice tutorial to get you up to speed if not). Namespaces, encapsulation, modularization, and bundling Modules in Python make it very easy to encapsulate components without polluting the global namespace. In contrast, Javascript in the browser will make everything a global if you are not careful[^2]. The good news is that it doesn't require too much extra effort to use a sort of module pattern in your Javascript code thanks to things like object literals and closures . Imagine you are writing some code to do some simple math operations that aren't in the Javascript Math library. Instead of doing this: // Don't do this function mean(x) { var i; var total = 0; for (i = 0; i < x.length; i++) { total += x[i]; } return total/x.length; } you should prefer putting your functions in an object: // This is better var mathlib = { mean: function (x) { var i; var total = 0; for (i = 0; i < x.length; i++) { total += x[i]; } return total/x.length; } }; An even better approach is to wrap your definitions in a closure to keep things better encapsulated: // This is even better! var mathlib = (function (lib) { lib.mean = function (x) { var i; var total = 0; for (i = 0; i < x.length; i++) { total += x[i]; } return total/x.length; }; return lib; })(mathlib || {}); This pattern allows for splitting components for a larger module into different files, which is often a good idea from the perspective of readability when things start getting more complex. Rather than go into further detail, I'll refer you to an excellent article on the module pattern in Javascript by Ben Cherry. Bundling HTTP/1.1 requires a new connection for every requested Javascript file. While this problem is rectified in HTTP/2, not many web servers and hosting providers support it yet as of late 2016. This has led to many different options for bundling multiple Javascript files into a single file which can be included in a web page with just one script tag. While these tools can be tempting to use, I strongly recommend avoiding them as much as possible (at least until you become more comfortable with the state of modern Javascript) for the following reasons: They require having Node.js installed. If you've read this far, you can probably handle that, but if your scientist colleagues aren't as experienced as you are with software development, asking them to have two entirely different language runtimes installed just to run the latest version of your helpful web interface may be asking a bit too much. The churn in Javascriptland is too fast-paced for those of us who have other things to spend our time on (although from my outsider's perspective, things appear to be settling down a bit lately). Bundling files makes Javascript debugging a bit more difficult insofar as it usually requires also building source maps. While most of the Javascript bundlers will do this for you if asked, it just adds to the overall complexity. The module pattern presented above is already sufficient in many cases. After all, what's a few milliseconds to load a second Javascript file from a server on your LAN? A good, (potentially) pure Python approach to bundling your Javascript files (for cases where it makes sense to split code into more than a single file) is the webassets module. Webassets offers a number of filters to run Javascript and CSS files through to accomplish tasks such as as minification and bundling. Here's a sample Tornado app: import tornado.web import tornado.ioloop from webassets import Environment class MainHandler(tornado.web.RequestHandler): def initialize(self, assets): self.assets = assets def get(self): self.render(\"index.html\", assets=assets) # Set up the webassets environment and make a bundle assets = Environment(directory=\".\", url=\"/static\") js_files = [\"mathlib.js\", \"thing.js\", \"class-example.js\"] assets.register(\"bundle\", *js_files, output=\"bundled.min.js\", filters=\"rjsmin\") # webassets ships with this filter included app = tornado.web.Application( [(r'/', MainHandler, dict(assets=assets))], static_path=\".\", template_path=\".\", debug=True) app.listen(8123) tornado.ioloop.IOLoop.current().start() To include the bundled file in the template, you would do something like this in the template: {% for url in assets['bundle'].urls() %} <script src=\"{{ url }}\"></script> {% end %} The careful reader may wonder why the for loop is used if all the Javascript files will be bundled into a single file in the end. This is because webassets has a helpful debug mode which eliminates the need for source mapping. By adding assets.debug = True to the Python file, assets['bundle'].urls() will return a list of all the original, uncompressed Javascript files. This results in individual script tags for each Javascript source file which makes debugging in the browser considerably easier at the expense of a (likely) small increase in startup time. There are a lot of nice features in webassets, though many of the filters require third-party tools (often using Node.js) to be installed. For this reason, I discourage using most of these until and unless you are comfortable with the rabbit hole of the Node world. ( Aside : Recently, I learned of the DukPy interpreter. While it's still early, it looks like a promising way of being able to include things that currently require Node-based tools while keeping everything purely Pythonic.) Embracing the present Among Python programmers, Javascript has a tendency to be considered a very poor programming language in terms of features and syntax. While this was once a more valid criticism, Javascript has steadily improved, and especially with the introduction of the ES2015 standard, it's a very comfortable language to work in[^3]. In this section, I will cover a few of the more useful features made available in ES2015 with the small caveat that using them requires using fairly up-to-date browsers (which is not normally a problem among scientists who in my experience are all using either Firefox or Chrome, anyway). Arrow functions In Python, self by convention refers to the instance of a class. This means that even with nested function definitions, the reference to self is always unambiguous, so you could do something like class Thing: def __init__(self, value): self.value = value def method(self): def function(): return self.value return function() and expect calling the method method on an instance of Thing to correctly return thing.value . In Javascript, the approximate equivalent of self is this which is by default bound as a reference to the function being called . In other words, var thing = { value: 1, method: function () { var func = function () { return this.value; }; return func(); } }; console.log(thing.method()); will print undefined because func has no value attribute. This can be fixed by explicitly binding this to func ( func = func.bind(this) ), but this quickly becomes cumbersome when the number of functions that need this fix grows. In part to simplify this, ES2015 introduced so-called arrow functions which are kind of like Python lambdas on steroids. One nice feature of arrow functions is that they lexically bind the this variable so we can rewrite the above to read var thing = { value: 1, method: function () { var func = () => this.value; return func(); } }; console.log(thing.method()); which correctly outputs 1 . Classes Prior to ES2015, classes in Javascript had to be implemented with a function: function MyClass(value) { this.value = value; } var instance = new MyClass(10); Implementing inheritance was cumbersome and required the use of the prototype attribute: function Programmer(language) { this.language = language; } // Add a method to the Programmer prototype Programmer.prototype.programThings = function () { console.log(\"Favorite language: \" + this.language); console.log(this instanceof Programmer); }; // Create child classes function PythonProgrammer() { Programmer.call(this); this.language = \"Python\"; } PythonProgrammer.prototype = Object.create(Programmer.prototype); function JavascriptProgrammer() { Programmer.call(this); this.language = \"Javascript\"; } JavascriptProgrammer.prototype = Object.create(Programmer.prototype); var pythonProgrammer = new PythonProgrammer(); var jsProgrammer = new JavascriptProgrammer(); pythonProgrammer.programThings(); jsProgrammer.programThings(); /* Output: Favorite language: Python true Favorite language: Javascript true */ With ES2015, classes can be defined in a more Pythonic way: class MyBaseClass { constructor(value) { this.value = value; } method() { return this.value; } } class MyNewClass extends MyBaseClass { secondMethod() { return this.value + 1; } } var instance = new MyNewClass(10); console.log(instance.method()); console.log(instance.secondMethod()); which outputs 10 11 Template strings Despite the Zen of Python suggesting that \"there should be one-- and preferably only one --obvious way to do it,\" there are 3 ways to format strings as of Python 3.6. The newest of these ways is the so-called \"f-string\" introduced by PEP 498 . This allows you to have variables dynamically inserted into strings without having to explicitly use %-formatting or the str.format method: x = 20 print(f'{x} is 20') ES2015 has a similar concept in template strings: var x = 20; console.log(`${x} is 20.`); Like Python docstrings, Javascript template strings can also span multiple lines: `this is ok even if it is a pointless string.` Iterators, generators, and promises One of the most useful features of Python is to be able to easily iterate over a list: for x in y: print(x) Javascript has long had, for example, Array.prototype.forEach() to iterate over an array, but this requires the use of a callback function to act on each value in the array. Javascript now has the more Pythonic for ... in and for ... of statements: var list = [1, 2, 3, 4, 5]; var obj = { a: 1, b: 2, c: 3 }; // for ... in on list makes x take on the *indeces* of list console.log(\"for (x in list)\"); for (let x in list) { console.log(x); } // for ... of on list makes x take on the *values* of list console.log(\"for (x of list)\"); for (let x of list) { console.log(x); } // for ... in on obj makes x take on the *keys* of obj console.log(\"for (x in obj)\"); for (let x in obj) { console.log(x); } Generators are used frequently in Python to efficiently iterate over values without having to pre-compute and store in memory each loop iteration's result. In Javascript, a function must explicitly be declared a generator by denoting it a function* and using the familiar yield keyword: function* itsATrap() { while (true) { yield \"What is it?\"; yield \"It's a trap!\"; } } var isItATrap = itsATrap(); console.log(isItATrap.next().value); console.log(isItATrap.next().value); console.log(isItATrap.next().value); console.log(isItATrap.next().value); Other newer features There are quite a few other nice things introduced by ES2015, but the above illustrates features that are perhaps the most welcomed by Python programmers. For a good overview of all newer Javascript features, see this , for example. Pythonic Javascript cheat sheet To summarize, what follows are a series of short snippets showing some common Pythonic concepts and their Javascript analogs. Exception handling try: thing() except Exception: print(\"oh no!\") raise ValueError(\"not a good value\") try { thing(); } catch (error) { console.error(\"oh no!\"); } throw \"not a good value\"; Iterators arr = [1, 2, 3] obj = { \"a\": 1, \"b\": 2, \"c\": 3 } for val in arr: print(val) for key in obj: print(key) var arr = [1, 2, 3]; var obj = { a: 1, b: 2, c: 3 }; for (let val of arr) { console.log(val); } // or... arr.forEach((value, index) => { console.log(value); }); for (let key in obj) { console.log(key); } Generators def gen(x): while True: yield x x = x + 1 function* gen(x) { while (true) { yield x; x++; } } Classes class Thing: def __init__(self, a): self.a = a def add_one(self): return self.a + 1 class OtherThing(Thing): def __init__(self, a, b): super(OtherThing, self).__init__(a) self.b = b def add_things(self): return self.a + self.b class Thing { constructor(a) { this.a = a; } addOne() { return this.a + 1; } } class OtherThing extends Thing { constructor(a, b) { super(a); this.b = b; } addThings() { return this.a + this.b; } } Functional programming Lambdas expression = lambda a, b: a + b // Arrow functions are more powerful than Python lambdas, but not in // this example! let expression = (a, b) => a + b; // or... let sameThing = function (a, b) { return a + b; } MapReduce from functools import reduce mapped = map(lambda a: a + 1, range(10)) print(reduce(lambda a, b: a + b, mapped)) let arr = []; for (let i = 0; i < 10; i++) { arr.push(i); } let mapped = arr.map((a) => a + 1); console.log(arr.reduce((a, b) => a + b)); Final thoughts These days, Javascript the language is much improved and potentially more Pythonic than ever before. Approaching a little Javascript from the perspective of a Python programmer, you can write good, clear code while avoiding many of the (mostly outdated) common pitfalls often brought up by Javascript detractors. [^1]: Not that modern Javascript tooling is really so good at reducing complexity... More on this later. [^2]: Of course, there are tools like webpack that can let you use modern Javascript modules in the browser, but this requires the step of bundling all the Javascript sources into a browser-friendly bundle. Unless you are willing to dive deep into the, um, interesting world of Javascript tooling, I recommend against this as you get started with more complex Javascript. [^3]: Although it would be a lot nicer if there were actually a standard library to speak of.","title":"Javascript for Python programmers"},{"location":"blog/javascript-for-python-programmers.html#namespaces-encapsulation-modularization-and-bundling","text":"Modules in Python make it very easy to encapsulate components without polluting the global namespace. In contrast, Javascript in the browser will make everything a global if you are not careful[^2]. The good news is that it doesn't require too much extra effort to use a sort of module pattern in your Javascript code thanks to things like object literals and closures . Imagine you are writing some code to do some simple math operations that aren't in the Javascript Math library. Instead of doing this: // Don't do this function mean(x) { var i; var total = 0; for (i = 0; i < x.length; i++) { total += x[i]; } return total/x.length; } you should prefer putting your functions in an object: // This is better var mathlib = { mean: function (x) { var i; var total = 0; for (i = 0; i < x.length; i++) { total += x[i]; } return total/x.length; } }; An even better approach is to wrap your definitions in a closure to keep things better encapsulated: // This is even better! var mathlib = (function (lib) { lib.mean = function (x) { var i; var total = 0; for (i = 0; i < x.length; i++) { total += x[i]; } return total/x.length; }; return lib; })(mathlib || {}); This pattern allows for splitting components for a larger module into different files, which is often a good idea from the perspective of readability when things start getting more complex. Rather than go into further detail, I'll refer you to an excellent article on the module pattern in Javascript by Ben Cherry.","title":"Namespaces, encapsulation, modularization, and bundling"},{"location":"blog/javascript-for-python-programmers.html#bundling","text":"HTTP/1.1 requires a new connection for every requested Javascript file. While this problem is rectified in HTTP/2, not many web servers and hosting providers support it yet as of late 2016. This has led to many different options for bundling multiple Javascript files into a single file which can be included in a web page with just one script tag. While these tools can be tempting to use, I strongly recommend avoiding them as much as possible (at least until you become more comfortable with the state of modern Javascript) for the following reasons: They require having Node.js installed. If you've read this far, you can probably handle that, but if your scientist colleagues aren't as experienced as you are with software development, asking them to have two entirely different language runtimes installed just to run the latest version of your helpful web interface may be asking a bit too much. The churn in Javascriptland is too fast-paced for those of us who have other things to spend our time on (although from my outsider's perspective, things appear to be settling down a bit lately). Bundling files makes Javascript debugging a bit more difficult insofar as it usually requires also building source maps. While most of the Javascript bundlers will do this for you if asked, it just adds to the overall complexity. The module pattern presented above is already sufficient in many cases. After all, what's a few milliseconds to load a second Javascript file from a server on your LAN? A good, (potentially) pure Python approach to bundling your Javascript files (for cases where it makes sense to split code into more than a single file) is the webassets module. Webassets offers a number of filters to run Javascript and CSS files through to accomplish tasks such as as minification and bundling. Here's a sample Tornado app: import tornado.web import tornado.ioloop from webassets import Environment class MainHandler(tornado.web.RequestHandler): def initialize(self, assets): self.assets = assets def get(self): self.render(\"index.html\", assets=assets) # Set up the webassets environment and make a bundle assets = Environment(directory=\".\", url=\"/static\") js_files = [\"mathlib.js\", \"thing.js\", \"class-example.js\"] assets.register(\"bundle\", *js_files, output=\"bundled.min.js\", filters=\"rjsmin\") # webassets ships with this filter included app = tornado.web.Application( [(r'/', MainHandler, dict(assets=assets))], static_path=\".\", template_path=\".\", debug=True) app.listen(8123) tornado.ioloop.IOLoop.current().start() To include the bundled file in the template, you would do something like this in the template: {% for url in assets['bundle'].urls() %} <script src=\"{{ url }}\"></script> {% end %} The careful reader may wonder why the for loop is used if all the Javascript files will be bundled into a single file in the end. This is because webassets has a helpful debug mode which eliminates the need for source mapping. By adding assets.debug = True to the Python file, assets['bundle'].urls() will return a list of all the original, uncompressed Javascript files. This results in individual script tags for each Javascript source file which makes debugging in the browser considerably easier at the expense of a (likely) small increase in startup time. There are a lot of nice features in webassets, though many of the filters require third-party tools (often using Node.js) to be installed. For this reason, I discourage using most of these until and unless you are comfortable with the rabbit hole of the Node world. ( Aside : Recently, I learned of the DukPy interpreter. While it's still early, it looks like a promising way of being able to include things that currently require Node-based tools while keeping everything purely Pythonic.)","title":"Bundling"},{"location":"blog/javascript-for-python-programmers.html#embracing-the-present","text":"Among Python programmers, Javascript has a tendency to be considered a very poor programming language in terms of features and syntax. While this was once a more valid criticism, Javascript has steadily improved, and especially with the introduction of the ES2015 standard, it's a very comfortable language to work in[^3]. In this section, I will cover a few of the more useful features made available in ES2015 with the small caveat that using them requires using fairly up-to-date browsers (which is not normally a problem among scientists who in my experience are all using either Firefox or Chrome, anyway).","title":"Embracing the present"},{"location":"blog/javascript-for-python-programmers.html#arrow-functions","text":"In Python, self by convention refers to the instance of a class. This means that even with nested function definitions, the reference to self is always unambiguous, so you could do something like class Thing: def __init__(self, value): self.value = value def method(self): def function(): return self.value return function() and expect calling the method method on an instance of Thing to correctly return thing.value . In Javascript, the approximate equivalent of self is this which is by default bound as a reference to the function being called . In other words, var thing = { value: 1, method: function () { var func = function () { return this.value; }; return func(); } }; console.log(thing.method()); will print undefined because func has no value attribute. This can be fixed by explicitly binding this to func ( func = func.bind(this) ), but this quickly becomes cumbersome when the number of functions that need this fix grows. In part to simplify this, ES2015 introduced so-called arrow functions which are kind of like Python lambdas on steroids. One nice feature of arrow functions is that they lexically bind the this variable so we can rewrite the above to read var thing = { value: 1, method: function () { var func = () => this.value; return func(); } }; console.log(thing.method()); which correctly outputs 1 .","title":"Arrow functions"},{"location":"blog/javascript-for-python-programmers.html#classes","text":"Prior to ES2015, classes in Javascript had to be implemented with a function: function MyClass(value) { this.value = value; } var instance = new MyClass(10); Implementing inheritance was cumbersome and required the use of the prototype attribute: function Programmer(language) { this.language = language; } // Add a method to the Programmer prototype Programmer.prototype.programThings = function () { console.log(\"Favorite language: \" + this.language); console.log(this instanceof Programmer); }; // Create child classes function PythonProgrammer() { Programmer.call(this); this.language = \"Python\"; } PythonProgrammer.prototype = Object.create(Programmer.prototype); function JavascriptProgrammer() { Programmer.call(this); this.language = \"Javascript\"; } JavascriptProgrammer.prototype = Object.create(Programmer.prototype); var pythonProgrammer = new PythonProgrammer(); var jsProgrammer = new JavascriptProgrammer(); pythonProgrammer.programThings(); jsProgrammer.programThings(); /* Output: Favorite language: Python true Favorite language: Javascript true */ With ES2015, classes can be defined in a more Pythonic way: class MyBaseClass { constructor(value) { this.value = value; } method() { return this.value; } } class MyNewClass extends MyBaseClass { secondMethod() { return this.value + 1; } } var instance = new MyNewClass(10); console.log(instance.method()); console.log(instance.secondMethod()); which outputs 10 11","title":"Classes"},{"location":"blog/javascript-for-python-programmers.html#template-strings","text":"Despite the Zen of Python suggesting that \"there should be one-- and preferably only one --obvious way to do it,\" there are 3 ways to format strings as of Python 3.6. The newest of these ways is the so-called \"f-string\" introduced by PEP 498 . This allows you to have variables dynamically inserted into strings without having to explicitly use %-formatting or the str.format method: x = 20 print(f'{x} is 20') ES2015 has a similar concept in template strings: var x = 20; console.log(`${x} is 20.`); Like Python docstrings, Javascript template strings can also span multiple lines: `this is ok even if it is a pointless string.`","title":"Template strings"},{"location":"blog/javascript-for-python-programmers.html#iterators-generators-and-promises","text":"One of the most useful features of Python is to be able to easily iterate over a list: for x in y: print(x) Javascript has long had, for example, Array.prototype.forEach() to iterate over an array, but this requires the use of a callback function to act on each value in the array. Javascript now has the more Pythonic for ... in and for ... of statements: var list = [1, 2, 3, 4, 5]; var obj = { a: 1, b: 2, c: 3 }; // for ... in on list makes x take on the *indeces* of list console.log(\"for (x in list)\"); for (let x in list) { console.log(x); } // for ... of on list makes x take on the *values* of list console.log(\"for (x of list)\"); for (let x of list) { console.log(x); } // for ... in on obj makes x take on the *keys* of obj console.log(\"for (x in obj)\"); for (let x in obj) { console.log(x); } Generators are used frequently in Python to efficiently iterate over values without having to pre-compute and store in memory each loop iteration's result. In Javascript, a function must explicitly be declared a generator by denoting it a function* and using the familiar yield keyword: function* itsATrap() { while (true) { yield \"What is it?\"; yield \"It's a trap!\"; } } var isItATrap = itsATrap(); console.log(isItATrap.next().value); console.log(isItATrap.next().value); console.log(isItATrap.next().value); console.log(isItATrap.next().value);","title":"Iterators, generators, and promises"},{"location":"blog/javascript-for-python-programmers.html#other-newer-features","text":"There are quite a few other nice things introduced by ES2015, but the above illustrates features that are perhaps the most welcomed by Python programmers. For a good overview of all newer Javascript features, see this , for example.","title":"Other newer features"},{"location":"blog/javascript-for-python-programmers.html#pythonic-javascript-cheat-sheet","text":"To summarize, what follows are a series of short snippets showing some common Pythonic concepts and their Javascript analogs.","title":"Pythonic Javascript cheat sheet"},{"location":"blog/javascript-for-python-programmers.html#exception-handling","text":"try: thing() except Exception: print(\"oh no!\") raise ValueError(\"not a good value\") try { thing(); } catch (error) { console.error(\"oh no!\"); } throw \"not a good value\";","title":"Exception handling"},{"location":"blog/javascript-for-python-programmers.html#iterators","text":"arr = [1, 2, 3] obj = { \"a\": 1, \"b\": 2, \"c\": 3 } for val in arr: print(val) for key in obj: print(key) var arr = [1, 2, 3]; var obj = { a: 1, b: 2, c: 3 }; for (let val of arr) { console.log(val); } // or... arr.forEach((value, index) => { console.log(value); }); for (let key in obj) { console.log(key); }","title":"Iterators"},{"location":"blog/javascript-for-python-programmers.html#generators","text":"def gen(x): while True: yield x x = x + 1 function* gen(x) { while (true) { yield x; x++; } }","title":"Generators"},{"location":"blog/javascript-for-python-programmers.html#classes_1","text":"class Thing: def __init__(self, a): self.a = a def add_one(self): return self.a + 1 class OtherThing(Thing): def __init__(self, a, b): super(OtherThing, self).__init__(a) self.b = b def add_things(self): return self.a + self.b class Thing { constructor(a) { this.a = a; } addOne() { return this.a + 1; } } class OtherThing extends Thing { constructor(a, b) { super(a); this.b = b; } addThings() { return this.a + this.b; } }","title":"Classes"},{"location":"blog/javascript-for-python-programmers.html#functional-programming","text":"","title":"Functional programming"},{"location":"blog/javascript-for-python-programmers.html#lambdas","text":"expression = lambda a, b: a + b // Arrow functions are more powerful than Python lambdas, but not in // this example! let expression = (a, b) => a + b; // or... let sameThing = function (a, b) { return a + b; }","title":"Lambdas"},{"location":"blog/javascript-for-python-programmers.html#mapreduce","text":"from functools import reduce mapped = map(lambda a: a + 1, range(10)) print(reduce(lambda a, b: a + b, mapped)) let arr = []; for (let i = 0; i < 10; i++) { arr.push(i); } let mapped = arr.map((a) => a + 1); console.log(arr.reduce((a, b) => a + b));","title":"MapReduce"},{"location":"blog/javascript-for-python-programmers.html#final-thoughts","text":"These days, Javascript the language is much improved and potentially more Pythonic than ever before. Approaching a little Javascript from the perspective of a Python programmer, you can write good, clear code while avoiding many of the (mostly outdated) common pitfalls often brought up by Javascript detractors. [^1]: Not that modern Javascript tooling is really so good at reducing complexity... More on this later. [^2]: Of course, there are tools like webpack that can let you use modern Javascript modules in the browser, but this requires the step of bundling all the Javascript sources into a browser-friendly bundle. Unless you are willing to dive deep into the, um, interesting world of Javascript tooling, I recommend against this as you get started with more complex Javascript. [^3]: Although it would be a lot nicer if there were actually a standard library to speak of.","title":"Final thoughts"},{"location":"blog/lmfit.html","text":"General-purpose fitting in Python can sometimes be a bit more challenging than one might at first suspect given the robust nature of tools like Numpy and Scipy. First we had leastsq . It works, although often requires a bit of manual tuning of initial guesses and always requires manual calculation of standard error from a covariance matrix (which isn't even one of the return values by default). Later we got curve_fit which is a bit more user friendly and even estimates and returns standard error for us by default! Alas, curve_fit is just a convenience wrapper on top of leastsq and suffers from some of the same general headaches. These days, we have the wonderful lmfit package. Not only can lmfit make fitting more user friendly, but it also is quite a bit more robust than using scipy directly. The documentation is thorough and rigorous, but that can also mean that it can be a bit overwhelming to get started with it. Here I work through a basic example in two slightly different ways in order to demonstrate how to use it. Generating the data Let's assume we have data that resembles a decaying sine wave (e.g., a damped oscillator). lmfit has quite a few pre-defined models, but this is not one of them. We can simulate the data with the following code: import numpy as np x = np.linspace(0, 10, 100) y = np.sin(5*x)*np.exp(-x/2.5) Real data is noisy, so let's add some noise: import numpy.random as npr y += npr.choice([-1, 1], size=y.shape)*npr.random(size=y.shape)/5 The resulting data: Using models The easiest way to work with lmfit is to ignore the lmfit.minimize function shown in the \"Getting Started\" section of the documentation and instead jump straight to the higher-level (and more useful) Model class. For one-time fitting, the lmfit.models.ExpressionModel class is provided. When creating a new ExpressionModel , you simply pass a string that is interpreted as a Python expression. For our decaying sine example, we might do this: import lmfit model = lmfit.models.ExpressionModel(\"ampl * sin((x - x0)*freq) * exp(-x/tau) + offset\") Let's make our initial guess for performing the fit under the constraint that the offset is fixed at 0: params = model.make_params(ampl=1, x0=0, freq=10, tau=1, offset=0) params[\"offset\"].set(vary=False) To fit, we pass the data and the parameters as arguments and the independent variable as a keyword argument: fit = model.fit(y, params, x=x) To visually check if the fit is good, lmfit provides both plot_fit and plot_residuals methods for model instances. The former shows the data, the initial guess, and its found best fit: We can also see the found parameters with standard errors and goodness of fit data with a fit report ( print(model.fit_report()) ): [[Model]] Model(_eval) [[Fit Statistics]] # function evals = 102 # data points = 100 # variables = 4 chi-square = 1.337 reduced chi-square = 0.014 Akaike info crit = -419.379 Bayesian info crit = -408.959 [[Variables]] ampl: 1.02147340 +/- 0.068013 (6.66%) (init= 1) offset: 0 (fixed) tau: 2.53669407 +/- 0.239335 (9.43%) (init= 1) x0: -0.00823894 +/- 0.012256 (148.76%) (init= 0) freq: 4.98932400 +/- 0.035399 (0.71%) (init= 10) [[Correlations]] (unreported correlations are < 0.100) C(ampl, tau) = -0.718 C(x0, freq) = 0.684 C(ampl, x0) = 0.139 Reusable models For improved reusability of models, a better approach is to subclass lmfit.models.Model directly. This allows us to implement a guess method to automate creating initial guesses. Following the pattern used in defining the models in the lmfit.models module, we can define our decaying sine model like so: class DecayingSineModel(lmfit.Model): def __init__(self, *args, **kwargs): def decaying_sine(x, ampl, offset, freq, x0, tau): return ampl * np.sin((x - x0)*freq) * np.exp(-x/tau) + offset super(DecayingSineModel, self).__init__(decaying_sine, *args, **kwargs) def guess(self, data, **kwargs): params = self.make_params() def pset(param, value): params[\"%s%s\" % (self.prefix, param)].set(value=value) pset(\"ampl\", np.max(data) - np.min(data)) pset(\"offset\", np.mean(data)) pset(\"freq\", 1) pset(\"x0\", 0) pset(\"tau\", 1) return lmfit.models.update_param_vals(params, self.prefix, **kwargs) Note that the point of the prefix is so that composite models can be constructed (the prefix prevents namespace clashes). Now we can fit as before but guess the starting parameters without thinking about it: model = DecayingSineModel() params = model.guess(y, x=x) fit = model.fit(y, params, x=x) which results in a similar fit as before: Extracting data from the fit In many cases we might want to extract parameters and standard error estimates programatically rather than by reading the fit report (e.g., if the fit will be used to produce a data point on another plot, then the standard error can be used for computing error bars). This is all included in the fit result via its params attribute. We can print the parameter values and errors like this: for key in fit.params: print(key, \"=\", fit.params[key].value, \"+/-\", fit.params[key].stderr) Final thoughts I've only scratched the surface of lmfit 's features, but the examples here demonstrate a good portion of the daily requirements of working with data from an experiment. As alluded to earlier, lmfit comes with many built-in models which makes it a pleasure to use for peak fitting (something that is often particularly difficult when using scipy directly). Finally, although lmfit can handle linear models just fine, I would instead recommend the statsmodels package. Using the power of pandas DataFrame s, models can be defined in a similar manner as with lmfit 's ExpressionModel s. A Jupyter notebook containing the above examples can be found here .","title":"Fitting with lmfit"},{"location":"blog/lmfit.html#generating-the-data","text":"Let's assume we have data that resembles a decaying sine wave (e.g., a damped oscillator). lmfit has quite a few pre-defined models, but this is not one of them. We can simulate the data with the following code: import numpy as np x = np.linspace(0, 10, 100) y = np.sin(5*x)*np.exp(-x/2.5) Real data is noisy, so let's add some noise: import numpy.random as npr y += npr.choice([-1, 1], size=y.shape)*npr.random(size=y.shape)/5 The resulting data:","title":"Generating the data"},{"location":"blog/lmfit.html#using-models","text":"The easiest way to work with lmfit is to ignore the lmfit.minimize function shown in the \"Getting Started\" section of the documentation and instead jump straight to the higher-level (and more useful) Model class. For one-time fitting, the lmfit.models.ExpressionModel class is provided. When creating a new ExpressionModel , you simply pass a string that is interpreted as a Python expression. For our decaying sine example, we might do this: import lmfit model = lmfit.models.ExpressionModel(\"ampl * sin((x - x0)*freq) * exp(-x/tau) + offset\") Let's make our initial guess for performing the fit under the constraint that the offset is fixed at 0: params = model.make_params(ampl=1, x0=0, freq=10, tau=1, offset=0) params[\"offset\"].set(vary=False) To fit, we pass the data and the parameters as arguments and the independent variable as a keyword argument: fit = model.fit(y, params, x=x) To visually check if the fit is good, lmfit provides both plot_fit and plot_residuals methods for model instances. The former shows the data, the initial guess, and its found best fit: We can also see the found parameters with standard errors and goodness of fit data with a fit report ( print(model.fit_report()) ): [[Model]] Model(_eval) [[Fit Statistics]] # function evals = 102 # data points = 100 # variables = 4 chi-square = 1.337 reduced chi-square = 0.014 Akaike info crit = -419.379 Bayesian info crit = -408.959 [[Variables]] ampl: 1.02147340 +/- 0.068013 (6.66%) (init= 1) offset: 0 (fixed) tau: 2.53669407 +/- 0.239335 (9.43%) (init= 1) x0: -0.00823894 +/- 0.012256 (148.76%) (init= 0) freq: 4.98932400 +/- 0.035399 (0.71%) (init= 10) [[Correlations]] (unreported correlations are < 0.100) C(ampl, tau) = -0.718 C(x0, freq) = 0.684 C(ampl, x0) = 0.139","title":"Using models"},{"location":"blog/lmfit.html#reusable-models","text":"For improved reusability of models, a better approach is to subclass lmfit.models.Model directly. This allows us to implement a guess method to automate creating initial guesses. Following the pattern used in defining the models in the lmfit.models module, we can define our decaying sine model like so: class DecayingSineModel(lmfit.Model): def __init__(self, *args, **kwargs): def decaying_sine(x, ampl, offset, freq, x0, tau): return ampl * np.sin((x - x0)*freq) * np.exp(-x/tau) + offset super(DecayingSineModel, self).__init__(decaying_sine, *args, **kwargs) def guess(self, data, **kwargs): params = self.make_params() def pset(param, value): params[\"%s%s\" % (self.prefix, param)].set(value=value) pset(\"ampl\", np.max(data) - np.min(data)) pset(\"offset\", np.mean(data)) pset(\"freq\", 1) pset(\"x0\", 0) pset(\"tau\", 1) return lmfit.models.update_param_vals(params, self.prefix, **kwargs) Note that the point of the prefix is so that composite models can be constructed (the prefix prevents namespace clashes). Now we can fit as before but guess the starting parameters without thinking about it: model = DecayingSineModel() params = model.guess(y, x=x) fit = model.fit(y, params, x=x) which results in a similar fit as before:","title":"Reusable models"},{"location":"blog/lmfit.html#extracting-data-from-the-fit","text":"In many cases we might want to extract parameters and standard error estimates programatically rather than by reading the fit report (e.g., if the fit will be used to produce a data point on another plot, then the standard error can be used for computing error bars). This is all included in the fit result via its params attribute. We can print the parameter values and errors like this: for key in fit.params: print(key, \"=\", fit.params[key].value, \"+/-\", fit.params[key].stderr)","title":"Extracting data from the fit"},{"location":"blog/lmfit.html#final-thoughts","text":"I've only scratched the surface of lmfit 's features, but the examples here demonstrate a good portion of the daily requirements of working with data from an experiment. As alluded to earlier, lmfit comes with many built-in models which makes it a pleasure to use for peak fitting (something that is often particularly difficult when using scipy directly). Finally, although lmfit can handle linear models just fine, I would instead recommend the statsmodels package. Using the power of pandas DataFrame s, models can be defined in a similar manner as with lmfit 's ExpressionModel s. A Jupyter notebook containing the above examples can be found here .","title":"Final thoughts"},{"location":"blog/matplotlib-color-cycle.html","text":"Matplotlib can be very easy to use at times, especially if you just want to make a simple \"y vs. x\" type of plot. But when it comes to specialized customization, it can be a bit challenging to find the proper solution. The situation is not helped by the fact that a lot of times, an obscure answer on Stack Overflow no longer works because the API changed. One common need is to color things in the same way. For example, say you want to plot two dependent variables with widely different scales that share an independent variable. This is often represented by having two separate vertical axes which are colored to match the lines or markers of each data set. The most basic approach is to manually assign colors for the lines and axes, but if using a custom style , such as the ggplot style, we need a way to access the color cycle used if we want to remain consistent with the selected style. Here is the best way I have found which works at least in version 1.5: colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] colors should now be a list which contains the colors defined in order by the current style.","title":"Getting Matplotlib's colors in order"},{"location":"blog/multiline-lambdas.html","text":"Although Python has anonymous lambda functions, they lack the flexibility that some languages such as Javascript or even modern C++ have. In Python, lambda functions are limited to a single statement, which is often interpreted as meaning that it can only do one thing. This is not strictly true, however, since constructing a tuple is considered a single statement. In other words, we can cheat a little and call two independent functions in one lambda like this: (lambda: (foo(), bar()))() This is still somewhat constraining since variables cannot be defined within the lambda expression, so cases where this trick is useful are limited. One instance where this is particularly nice though is when defining callbacks for a GUI. Typically when a user clicks on a button, there might be several actions that should be triggered, such as starting an experiment, updating a GUI label, etc. Below is a simple example to illustrate this method: import sys from PyQt5.QtWidgets import * from PyQt5.QtGui import * class MainWindow(QWidget): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.setWindowTitle('Multiline lambdas') self.label = QLabel(\"Click the button\") self.button = QPushButton(\"Click me!\") self.button.clicked.connect(lambda: ( self.button.setEnabled(False), self.button.setText(\"You clicked me!\"), self.label.setText(\"Woo!\") )) layout = QVBoxLayout() layout.addWidget(self.label) layout.addWidget(self.button) self.setLayout(layout) if __name__ == \"__main__\": app = QApplication(sys.argv) win = MainWindow() win.show() sys.exit(app.exec_())","title":"Multiline lambdas"},{"location":"blog/postgres-time-series-database.html","text":"Time series databases (TSDBs) are quite popular these days. To name a few, there are InfluxDB , Graphite , Druid , Kairos , and Prometheus . All aim to optimize data storage and querying for time-based data, which is highly relevant in a physics labs where there are multitude of \"metrics\" (to borrow a phrase used frequently in TSDB documentation) that naturally lend themselves to time series representation: lab (and individual device) temperatures, vacuum chamber pressures, and laser powers, just to name a few. Ideally, one could log various data to one of these databases and then use a tool like Grafana to visualize it. Sadly, more traditional relational databases like SQLite and PostgreSQL are not (currently) supported by Grafana (although this is now being addressed by a datasource plugin in development). Nevertheless, there are quite a few reasons to favor a traditional RDBMS over a newfangled TSDB. To name a few: Longevity: SQL has been around since the 1970s and became standardized in the 1980s. Ubiquity: almost every server (web or otherwise) has an instance of SQL installed. If not, SQLite doesn't even require a server! Community: not to suggest there aren't good communities with TSDBs, but the Postgres and SQLite communities in particular are generally quite helpful. Combined with the longevity aspect, any question one may have about how to accomplish a particular task with a SQL database is likely to be easily answerable with a simple web search. In this post, I will outline a few things I have learned in using SQL for storing time series data. In particular, I will focus on Postgres, but the same general principles apply to other dialects. Sample code for some examples can be found on GitLab . Schema definition One \"disadvantage\" to SQL is it traditionally requires tightly defined schema. In practice when logging time series data, this is not usually a problem since each measurement device can neatly have its own column. Where this can become somewhat of nuiscance is when adding new devices. InfluxDB (for example) gets around this with its query language being quite flexible. In traditional SQL, the approach would require altering a table to add a new column. This is not too difficult in principle, but requires a (naive) program for logging data to frequently make ALTER TABLE calls and check if columns already exist. (Note that if using Python, this can be easily dealt with by using the dataset library.). In real laboratories, though, we tend to know the kinds of things we are going to measure. So even if we add new devices that we want to log data from, we can still come up with a reasonable schema definition that fits well within the SQL paradigm. As an example, let's consider storing data from thermocouples in a table. We could get away with as few as three columns to describe the data: a timestamp (of course), a name or unique ID of the sensor, and a temperature measurement. For good measure, we should also add a primary key ID column to make a grand total of four columns. So far, our table looks like this: id | timestamp | sensor | temperature ----+-------------------------------+-----------+------------- For the timestamp column, I highly recommend using TIMESTAMP WITH TIME ZONE rather than TIMESTAMP WITHOUT TIME ZONE (more on why later). For efficient querying, we'll want to index the timestamp and sensor columns. Depending on the number of sensors, it may also make sense to make a combined index on both, but we can defer this decision to later if it becomes necessary. Using SQLAlchemy , we define our table like this: metadata = sa.MetaData() table = sa.Table( 'timeseries', metadata, sa.Column('id', sa.Integer, primary_key=True), sa.Column('timestamp', sa.DateTime(timezone=True), nullable=False, index=True), sa.Column('sensor', sa.String(length=128), nullable=False, index=True), sa.Column('temperature', sa.Float(precision=4), nullable=False)) metadata.create_all(bind=engine) which results in the following SQL: CREATE TABLE timeseries ( id SERIAL NOT NULL, timestamp TIMESTAMP WITH TIME ZONE NOT NULL, sensor VARCHAR(128) NOT NULL, temperature FLOAT(4) NOT NULL, PRIMARY KEY (id) ); CREATE INDEX ix_timeseries_sensor ON timeseries (sensor); CREATE INDEX ix_timeseries_timestamp ON timeseries (timestamp); Basic querying Simple queries are performed as normal: SELECT * FROM timeseries WHERE sensor = 'sensor_01'; Postgres has quite a few date and time functions for building more complicated queries. It understands ISO 8601 out of the box: test=> SELECT * FROM timeseries test-> WHERE timestamp > '2016-06-13T22:00+02' test-> AND sensor = 'sensor_01'; id | timestamp | sensor | temperature ----+-------------------------------+-----------+------------- 8 | 2016-06-14 23:18:16.149606+02 | sensor_01 | 22.7061 4 | 2016-06-14 23:18:11.985645+02 | sensor_01 | 25.4643 (2 rows) Here we are explicit with the UTC offset of +2 hours (CEST). If omitted, the server locale is assumed. This brings us to why we should bother with time zones in the first place: internally, we want all timestamps stored in UTC to avoid ambiguity (Postgres already does this internally). Externally, (e.g., from Python scripts), we want to be able to use whatever time zone we're in to not have to think too hard. SQLAlchemy treats naive datetime objects, uh, naively. This means that if a new datetime is created without explicitly specifying a time zone, that +2 hours above is lost and our time quries will start to get confusing. To avoid this problem, the best solution I have found is to always declare columns as TIMESTAMP WITH TIME ZONE ( DateTime(timezone=True) in SQLAlchemy terms) and explicitly. So rather than inserting new timestamps with from datetime import datetime # ... timestamp = datetime.now() instead prefer from datetime import datetime, timezone # ... timestamp = datetime.now(timezone.utc) Aside: why oh why doesn't datetime.utcnow just do this? Now we can build queries in Python like this using pandas and raw SQL queries: today = datetime.now(timezone.utc) today = today.replace(hour=0, minute=0, second=0, microsecond=0) query = ( \"SELECT * FROM timeseries \" + \"WHERE timestamp >= '{}' \".format(today.isoformat()) + \"AND sensor = 'sensor_01'\" ) df = pd.read_sql_query(query, engine, index_col=\"timestamp\") Data aggregation Depending on data density, it may be useful to downsample data and look at aggregates such as the mean temperature in half-hour windows over the course of a day. We can easily accomplish this after the fact with pandas, but we can just as easily use Postgres aggregate functions to do this for us on the server. One advantage to this approach is a reduction in network overhead, which is especially relevant for very large datasets. Another is that these queries can be cached using materialized views . (This is a more advanced topic that I will not cover here. Instead, see the link in the references section below for a good treatment). The key here is to use the date_trunc aggregate function and GROUP BY to only look at (for example) one hour at a time. An example of an aggregate query: SELECT date_trunc('hour', timestamp) AS timestamp, avg(temperature) AS temperature FROM timeseries WHERE timestamp >= '2016-06-25' AND sensor = 'sensor_01' GROUP BY date_trunc('hour', timestamp) ORDER BY timestamp; which results in something like: timestamp | temperature ------------------------+------------------ 2016-06-25 00:00:00+02 | 22.0828623312065 2016-06-25 01:00:00+02 | 22.0026334276975 2016-06-25 02:00:00+02 | 21.9871146672498 2016-06-25 03:00:00+02 | 22.0274553065207 2016-06-25 04:00:00+02 | 21.9357200048187 2016-06-25 05:00:00+02 | 21.9737668623899 2016-06-25 06:00:00+02 | 22.0098525849685 2016-06-25 07:00:00+02 | 22.0767008988982 2016-06-25 08:00:00+02 | 22.2146511332874 2016-06-25 09:00:00+02 | 21.9118559617263 2016-06-25 10:00:00+02 | 22.0417969508838 2016-06-25 11:00:00+02 | 22.0554379473676 2016-06-25 12:00:00+02 | 22.0193907419841 2016-06-25 13:00:00+02 | 22.0560295554413 2016-06-25 14:00:00+02 | 21.8087244594798 2016-06-25 15:00:00+02 | 22.0494429762518 2016-06-25 16:00:00+02 | 21.9082782661007 2016-06-25 17:00:00+02 | 21.4403478373652 (18 rows) Other strategies Another approach to avoid the time zone issue entirely is to simply store timestamps using something like UNIX time. Since pretty much every programming language imaginable has a built-in function to return time in seconds since the epoch, this is a reasonable approach (and is a bit more portable to other SQL dialects). The major downside to this is that compared to ISO 8601, UNIX time is not as readable by humans and therefore may require extra steps to convert to and from a human-readable format. Depending on what you are doing with your data, it could also make sense to store, say, an hour's worth of data in a single row using the ARRAY data type. Combining arrays with array functions could then effectively do aggregation (somewhat) automatically rather than by query. This could also mean a bit of extra work when inserting new data or getting data stored in the database into a form friendly to your data analysis tools of choice. References and further reading Querying Time Series in Postgresql Materialized View Strategies Using PostgreSQL","title":"Using Postgres as a time series database"},{"location":"blog/postgres-time-series-database.html#schema-definition","text":"One \"disadvantage\" to SQL is it traditionally requires tightly defined schema. In practice when logging time series data, this is not usually a problem since each measurement device can neatly have its own column. Where this can become somewhat of nuiscance is when adding new devices. InfluxDB (for example) gets around this with its query language being quite flexible. In traditional SQL, the approach would require altering a table to add a new column. This is not too difficult in principle, but requires a (naive) program for logging data to frequently make ALTER TABLE calls and check if columns already exist. (Note that if using Python, this can be easily dealt with by using the dataset library.). In real laboratories, though, we tend to know the kinds of things we are going to measure. So even if we add new devices that we want to log data from, we can still come up with a reasonable schema definition that fits well within the SQL paradigm. As an example, let's consider storing data from thermocouples in a table. We could get away with as few as three columns to describe the data: a timestamp (of course), a name or unique ID of the sensor, and a temperature measurement. For good measure, we should also add a primary key ID column to make a grand total of four columns. So far, our table looks like this: id | timestamp | sensor | temperature ----+-------------------------------+-----------+------------- For the timestamp column, I highly recommend using TIMESTAMP WITH TIME ZONE rather than TIMESTAMP WITHOUT TIME ZONE (more on why later). For efficient querying, we'll want to index the timestamp and sensor columns. Depending on the number of sensors, it may also make sense to make a combined index on both, but we can defer this decision to later if it becomes necessary. Using SQLAlchemy , we define our table like this: metadata = sa.MetaData() table = sa.Table( 'timeseries', metadata, sa.Column('id', sa.Integer, primary_key=True), sa.Column('timestamp', sa.DateTime(timezone=True), nullable=False, index=True), sa.Column('sensor', sa.String(length=128), nullable=False, index=True), sa.Column('temperature', sa.Float(precision=4), nullable=False)) metadata.create_all(bind=engine) which results in the following SQL: CREATE TABLE timeseries ( id SERIAL NOT NULL, timestamp TIMESTAMP WITH TIME ZONE NOT NULL, sensor VARCHAR(128) NOT NULL, temperature FLOAT(4) NOT NULL, PRIMARY KEY (id) ); CREATE INDEX ix_timeseries_sensor ON timeseries (sensor); CREATE INDEX ix_timeseries_timestamp ON timeseries (timestamp);","title":"Schema definition"},{"location":"blog/postgres-time-series-database.html#basic-querying","text":"Simple queries are performed as normal: SELECT * FROM timeseries WHERE sensor = 'sensor_01'; Postgres has quite a few date and time functions for building more complicated queries. It understands ISO 8601 out of the box: test=> SELECT * FROM timeseries test-> WHERE timestamp > '2016-06-13T22:00+02' test-> AND sensor = 'sensor_01'; id | timestamp | sensor | temperature ----+-------------------------------+-----------+------------- 8 | 2016-06-14 23:18:16.149606+02 | sensor_01 | 22.7061 4 | 2016-06-14 23:18:11.985645+02 | sensor_01 | 25.4643 (2 rows) Here we are explicit with the UTC offset of +2 hours (CEST). If omitted, the server locale is assumed. This brings us to why we should bother with time zones in the first place: internally, we want all timestamps stored in UTC to avoid ambiguity (Postgres already does this internally). Externally, (e.g., from Python scripts), we want to be able to use whatever time zone we're in to not have to think too hard. SQLAlchemy treats naive datetime objects, uh, naively. This means that if a new datetime is created without explicitly specifying a time zone, that +2 hours above is lost and our time quries will start to get confusing. To avoid this problem, the best solution I have found is to always declare columns as TIMESTAMP WITH TIME ZONE ( DateTime(timezone=True) in SQLAlchemy terms) and explicitly. So rather than inserting new timestamps with from datetime import datetime # ... timestamp = datetime.now() instead prefer from datetime import datetime, timezone # ... timestamp = datetime.now(timezone.utc) Aside: why oh why doesn't datetime.utcnow just do this? Now we can build queries in Python like this using pandas and raw SQL queries: today = datetime.now(timezone.utc) today = today.replace(hour=0, minute=0, second=0, microsecond=0) query = ( \"SELECT * FROM timeseries \" + \"WHERE timestamp >= '{}' \".format(today.isoformat()) + \"AND sensor = 'sensor_01'\" ) df = pd.read_sql_query(query, engine, index_col=\"timestamp\")","title":"Basic querying"},{"location":"blog/postgres-time-series-database.html#data-aggregation","text":"Depending on data density, it may be useful to downsample data and look at aggregates such as the mean temperature in half-hour windows over the course of a day. We can easily accomplish this after the fact with pandas, but we can just as easily use Postgres aggregate functions to do this for us on the server. One advantage to this approach is a reduction in network overhead, which is especially relevant for very large datasets. Another is that these queries can be cached using materialized views . (This is a more advanced topic that I will not cover here. Instead, see the link in the references section below for a good treatment). The key here is to use the date_trunc aggregate function and GROUP BY to only look at (for example) one hour at a time. An example of an aggregate query: SELECT date_trunc('hour', timestamp) AS timestamp, avg(temperature) AS temperature FROM timeseries WHERE timestamp >= '2016-06-25' AND sensor = 'sensor_01' GROUP BY date_trunc('hour', timestamp) ORDER BY timestamp; which results in something like: timestamp | temperature ------------------------+------------------ 2016-06-25 00:00:00+02 | 22.0828623312065 2016-06-25 01:00:00+02 | 22.0026334276975 2016-06-25 02:00:00+02 | 21.9871146672498 2016-06-25 03:00:00+02 | 22.0274553065207 2016-06-25 04:00:00+02 | 21.9357200048187 2016-06-25 05:00:00+02 | 21.9737668623899 2016-06-25 06:00:00+02 | 22.0098525849685 2016-06-25 07:00:00+02 | 22.0767008988982 2016-06-25 08:00:00+02 | 22.2146511332874 2016-06-25 09:00:00+02 | 21.9118559617263 2016-06-25 10:00:00+02 | 22.0417969508838 2016-06-25 11:00:00+02 | 22.0554379473676 2016-06-25 12:00:00+02 | 22.0193907419841 2016-06-25 13:00:00+02 | 22.0560295554413 2016-06-25 14:00:00+02 | 21.8087244594798 2016-06-25 15:00:00+02 | 22.0494429762518 2016-06-25 16:00:00+02 | 21.9082782661007 2016-06-25 17:00:00+02 | 21.4403478373652 (18 rows)","title":"Data aggregation"},{"location":"blog/postgres-time-series-database.html#other-strategies","text":"Another approach to avoid the time zone issue entirely is to simply store timestamps using something like UNIX time. Since pretty much every programming language imaginable has a built-in function to return time in seconds since the epoch, this is a reasonable approach (and is a bit more portable to other SQL dialects). The major downside to this is that compared to ISO 8601, UNIX time is not as readable by humans and therefore may require extra steps to convert to and from a human-readable format. Depending on what you are doing with your data, it could also make sense to store, say, an hour's worth of data in a single row using the ARRAY data type. Combining arrays with array functions could then effectively do aggregation (somewhat) automatically rather than by query. This could also mean a bit of extra work when inserting new data or getting data stored in the database into a form friendly to your data analysis tools of choice.","title":"Other strategies"},{"location":"blog/postgres-time-series-database.html#references-and-further-reading","text":"Querying Time Series in Postgresql Materialized View Strategies Using PostgreSQL","title":"References and further reading"},{"location":"blog/rpi-usb-ethernet-gateway.html","text":"Introduction One of the most convenient ways of communicating with experimental devices (such as oscilloscopes, frequency generators, pulse generators, etc.) is via ethernet. The advantages of this over other forms of communication such as GPIB, RS-232 serial ports, etc., is that, provided the device receives a fixed IP address or some sort of dynamic DNS service is used, it doesn't matter where it is located and specialty cabling can be kept to a minimum. Luckily, most of these devices, even if they are not equipped with ethernet capability, can be made to work over ethernet with some sort of device server (e.g., there are device servers such as those made by Moxa which can \"convert\" RS-232 serial port communications to ethernet). A lot of modern devices come equipped with a USB port on the back which complies with the USBTMC (USB test and measurement class) specifications. Even fairly inexpensive equipment which lacks an ethernet port are likely to have a USB port for USBTMC communications (e.g., the popular and inexpensive Rigol DS1000D series digital oscilloscopes). There exists a USBTMC Linux kernel module which allows for communication with USBTMC devices via /dev/usbtmcNNN device files. This module, coupled with the versatile socat command, can thus allow for transparent communications over ethernet with a USBTMC device as if it were connected via ethernet itself. The rest of this note describes the process for using a Raspberry Pi as a USBTMC to ethernet adapter. Compiling the RPi kernel The RPi's default kernel does not include USBTMC support as a module or built into the kernel. This requires building from scratch, the full details of which can be found here . The basic idea is to grab the RPi kernel source on a fast computer and cross compile it with the USBTMC kernel module[^1] (or build into the kernel if you prefer). There are a few caveats and pitfalls, so the following provides the step-by-step approach that worked for me. To get started on a 64-bit Linux machine, make sure you have the 32-bit libraries installed. On Debian and derivatives : sudo dpkg --add-architecture i386 # enable multi-arch sudo apt-get update sudo apt-get install ia32-libs Once this is done, the following steps will get things working: Get the RPi kernel source: git init git clone --depth 1 git://github.com/raspberrypi/linux.git Get the compiler for cross-compiling: git clone git://github.com/raspberrypi/tools.git Compile the kernel In the kernel source directory, do: make mrproper Next, copy the default configuration file: cp arch/arm/configs/bcmrpi_defconfig .config Select the appropriate compiler to use by defining an environment variable that points to the right place (TODO: put the right thing here): export CCPREFIX=/path/to/your/compiler/binary/prefix-of-binary- Pre-configure everything and accept the defaults : yes \"\" | make oldconfig Now we can enable building of the usbtmc kernel module. Run make menuconfig . Navigate to Device Drivers > USB support > USB Test and Measurement Class support and make sure it is marked M to build a module. Save the configuration file, then exit. Now build the kernel: make ARCH=arm CROSS_COMPILE=${CCPREFIX} -jN where N is the number of CPU cores + 1 (e.g., if there are 4 cores, N = 5). This step will take several minutes on a reasonably fast computer. Next build the modules: make ARCH=arm CROSS_COMPILE=${CCPREFIX} modules Transferring the kernel: First copy to the RPi: scp arch/arm/boot/Image pi@yourpi:kernel_someuniqueid.img Then on the RPi, copy this over to /boot : sudo cp kernel_someuniqueid.img /boot Edit the bootloader configuration file to use the new kernel by making sure the following line appears: kernel=kernel_someuniqueid.img and comment out any other kernel=... lines. Transferring the kernel modules: On the build machine, make a temporary directory to install modules to: mkdir ~/modules export MODULES_TEMP=~/modules In the build directory: make ARCH=arm CROSS_COMPILE=${CCPREFIX} INSTALL_MOD_PATH=${MODULES_TEMP} modules_install Now in the temporary directory, there should be a lib directory. We don't need the source/headers, so remove them (otherwise you might run out of space on the RPi SD card!). Transfer these over to the RPi: scp -r lib pi@yourpi: On the RPi, copy and overwrite the contents of lib into /lib : sudo cp -f lib/* /lib Only do this step while running a different version of the kernel than what you compiled! Reboot. Load the module: sudo modprobe usbtmc Connect the USB device. There should now be a device named /dev/usbtmc0 . Talking to the device A Python script for piping data to and from a USBTMC device can be found here . It should be run through socat which does the more difficult work of properly transferring packets. The socat command I use is socat tcp-listen:5025,fork,reuseaddr,crnl,tcpwrap=script\\ EXEC:\"python usbtmc_pipe.py\",su-d=pi,pty,echo=0 Other notes It turns out that it is not necessary to use the kernel module to talk to USBTMC devices. A pure Python implementation of using the USBTMC protocol also exists. This has the advantage of not requiring a custom kernel for the RPi, but it adds the slight complexity of needing to specify vendor and product IDs. Footnotes [^1]: In the make menuconfig configuration menus, the option can be found under Device Drivers > USB support > USB Test and Measurement Class support .","title":"Raspberry Pi as a USB to Ethernet Gateway"},{"location":"blog/rpi-usb-ethernet-gateway.html#introduction","text":"One of the most convenient ways of communicating with experimental devices (such as oscilloscopes, frequency generators, pulse generators, etc.) is via ethernet. The advantages of this over other forms of communication such as GPIB, RS-232 serial ports, etc., is that, provided the device receives a fixed IP address or some sort of dynamic DNS service is used, it doesn't matter where it is located and specialty cabling can be kept to a minimum. Luckily, most of these devices, even if they are not equipped with ethernet capability, can be made to work over ethernet with some sort of device server (e.g., there are device servers such as those made by Moxa which can \"convert\" RS-232 serial port communications to ethernet). A lot of modern devices come equipped with a USB port on the back which complies with the USBTMC (USB test and measurement class) specifications. Even fairly inexpensive equipment which lacks an ethernet port are likely to have a USB port for USBTMC communications (e.g., the popular and inexpensive Rigol DS1000D series digital oscilloscopes). There exists a USBTMC Linux kernel module which allows for communication with USBTMC devices via /dev/usbtmcNNN device files. This module, coupled with the versatile socat command, can thus allow for transparent communications over ethernet with a USBTMC device as if it were connected via ethernet itself. The rest of this note describes the process for using a Raspberry Pi as a USBTMC to ethernet adapter.","title":"Introduction"},{"location":"blog/rpi-usb-ethernet-gateway.html#compiling-the-rpi-kernel","text":"The RPi's default kernel does not include USBTMC support as a module or built into the kernel. This requires building from scratch, the full details of which can be found here . The basic idea is to grab the RPi kernel source on a fast computer and cross compile it with the USBTMC kernel module[^1] (or build into the kernel if you prefer). There are a few caveats and pitfalls, so the following provides the step-by-step approach that worked for me. To get started on a 64-bit Linux machine, make sure you have the 32-bit libraries installed. On Debian and derivatives : sudo dpkg --add-architecture i386 # enable multi-arch sudo apt-get update sudo apt-get install ia32-libs Once this is done, the following steps will get things working:","title":"Compiling the RPi kernel"},{"location":"blog/rpi-usb-ethernet-gateway.html#get-the-rpi-kernel-source","text":"git init git clone --depth 1 git://github.com/raspberrypi/linux.git","title":"Get the RPi kernel source:"},{"location":"blog/rpi-usb-ethernet-gateway.html#get-the-compiler-for-cross-compiling","text":"git clone git://github.com/raspberrypi/tools.git Compile the kernel In the kernel source directory, do: make mrproper Next, copy the default configuration file: cp arch/arm/configs/bcmrpi_defconfig .config Select the appropriate compiler to use by defining an environment variable that points to the right place (TODO: put the right thing here): export CCPREFIX=/path/to/your/compiler/binary/prefix-of-binary- Pre-configure everything and accept the defaults : yes \"\" | make oldconfig Now we can enable building of the usbtmc kernel module. Run make menuconfig . Navigate to Device Drivers > USB support > USB Test and Measurement Class support and make sure it is marked M to build a module. Save the configuration file, then exit. Now build the kernel: make ARCH=arm CROSS_COMPILE=${CCPREFIX} -jN where N is the number of CPU cores + 1 (e.g., if there are 4 cores, N = 5). This step will take several minutes on a reasonably fast computer. Next build the modules: make ARCH=arm CROSS_COMPILE=${CCPREFIX} modules","title":"Get the compiler for cross-compiling:"},{"location":"blog/rpi-usb-ethernet-gateway.html#transferring-the-kernel","text":"First copy to the RPi: scp arch/arm/boot/Image pi@yourpi:kernel_someuniqueid.img Then on the RPi, copy this over to /boot : sudo cp kernel_someuniqueid.img /boot Edit the bootloader configuration file to use the new kernel by making sure the following line appears: kernel=kernel_someuniqueid.img and comment out any other kernel=... lines.","title":"Transferring the kernel:"},{"location":"blog/rpi-usb-ethernet-gateway.html#transferring-the-kernel-modules","text":"On the build machine, make a temporary directory to install modules to: mkdir ~/modules export MODULES_TEMP=~/modules In the build directory: make ARCH=arm CROSS_COMPILE=${CCPREFIX} INSTALL_MOD_PATH=${MODULES_TEMP} modules_install Now in the temporary directory, there should be a lib directory. We don't need the source/headers, so remove them (otherwise you might run out of space on the RPi SD card!). Transfer these over to the RPi: scp -r lib pi@yourpi: On the RPi, copy and overwrite the contents of lib into /lib : sudo cp -f lib/* /lib Only do this step while running a different version of the kernel than what you compiled!","title":"Transferring the kernel modules:"},{"location":"blog/rpi-usb-ethernet-gateway.html#reboot","text":"","title":"Reboot."},{"location":"blog/rpi-usb-ethernet-gateway.html#load-the-module","text":"sudo modprobe usbtmc","title":"Load the module:"},{"location":"blog/rpi-usb-ethernet-gateway.html#connect-the-usb-device","text":"There should now be a device named /dev/usbtmc0 .","title":"Connect the USB device."},{"location":"blog/rpi-usb-ethernet-gateway.html#talking-to-the-device","text":"A Python script for piping data to and from a USBTMC device can be found here . It should be run through socat which does the more difficult work of properly transferring packets. The socat command I use is socat tcp-listen:5025,fork,reuseaddr,crnl,tcpwrap=script\\ EXEC:\"python usbtmc_pipe.py\",su-d=pi,pty,echo=0","title":"Talking to the device"},{"location":"blog/rpi-usb-ethernet-gateway.html#other-notes","text":"It turns out that it is not necessary to use the kernel module to talk to USBTMC devices. A pure Python implementation of using the USBTMC protocol also exists. This has the advantage of not requiring a custom kernel for the RPi, but it adds the slight complexity of needing to specify vendor and product IDs.","title":"Other notes"},{"location":"blog/rpi-usb-ethernet-gateway.html#footnotes","text":"[^1]: In the make menuconfig configuration menus, the option can be found under Device Drivers > USB support > USB Test and Measurement Class support .","title":"Footnotes"},{"location":"blog/simplifying-argparse.html","text":"One of the best things about Python is its standard library: it's frequently possible to create complex applications while requiring few (if any) external dependencies. For example, command line interfaces can be easily built with the argparse module. Despite this, there exist several alternative, third-party modules (e.g., docopt , click , and begins ). These all tend to share similar motivations: while argparse is powerful, it is by inherently verbose and is therefore cumbersome to use for more complex CLIs which use advanced features such as subcommands. Nevertheless, I tend to prefer sticking with argparse in part because I am already familiar with the API and because using it means I don't need to bring in another dependency from PyPI just to add a small bit of extra functionality. The good news is that with a simple decorator and a convenience function, writing CLIs with subcommands in argparse is pretty trivial and clean. Start by creating a parser and subparsers in cli.py : from argparse import ArgumentParser cli = ArgumentParser() subparsers = cli.add_subparsers(dest=\"subcommand\") Note that we are storing the name of the called subcommand so that we can later print help if either no subcommand is given or if an unrecognized one is. Now we can define a decorator to turn a function into a subcommand: def subcommand(args=[], parent=subparsers): def decorator(func): parser = parent.add_parser(func.__name__, description=func.__doc__) for arg in args: parser.add_argument(*arg[0], **arg[1]) parser.set_defaults(func=func) return decorator What this does is take the wrapped function and use its name and docstring for the subcommand name and help string, respectively. Next it automatically adds arguments for the subcommand from a list passed to the decorator. In order to dispatch the command later, the usual parser.set_defaults method is used to store the function itself in the func variable. In the simplest case, we can create a subcommand which requires no arguments as follows: @subcommand() def nothing(args): print(\"Nothing special!\") Meanwhile, in our main function, we dispatch the subcommand as follows: if __name__ == \"__main__\": args = cli.parse_args() if args.subcommand is None: cli.print_help() else: args.func(args) Now running python cli.py nothing will run the nothing function and simply print Nothing special! to stdout. More often, subcommands require their own set of options. In the definition of the subcommand decorator above, these options can be given as a list of length-2 lists that contain the name or flags for the argument and all keyword arguments used by ArgumentParser.add_argument . This is a bit cumbersome as is, so it's useful to define a small helper function that takes arguments just like ArgumentParser.add_argument : def argument(*name_or_flags, **kwargs): return ([*name_or_flags], kwargs) Now we can define commands with arguments like so: @subcommand([argument(\"-d\", help=\"Debug mode\", action=\"store_true\")]) def test(args): print(args) @subcommand([argument(\"-f\", \"--filename\", help=\"A thing with a filename\")]) def filename(args): print(args.filename) @subcommand([argument(\"name\", help=\"Name\")]) def name(args): print(args.name) That's all there is to it! Quite a bit better than the default way to build a CLI with subcommands. The full example can be found here .","title":"Simplifying argparse usage with subcommands"},{"location":"blog/sudo-ssh-auth.html","text":"For best security on a public system, it is generally best to disable password-based logins with ssh and instead require authorized keys. However, this complicates things if you want to use sudo with a regular user account, since by default it uses the standard system password to verify the user is authorized to run commands as root. Enter pam_ssh_agent_auth . This module allows using regular ssh keys and ssh-agent to verify the user has the proper authorization to use sudo . Prerequisites You'll want to start by ensuring you have generated ssh keys for your user and are using ssh-agent . To generate the keys: $ ssh-keygen Then just accept the defaults, but make sure to set a password for your new key pair. Add the public key to $HOME/.ssh/authorized_keys . Installation Since the PAM module isn't in Debian, first grab the build dependencies: # apt-get install build-essential checkinstall libssl-dev libpam0g-dev Next, grab the source and build: # wget http://downloads.sourceforge.net/project/pamsshagentauth/pam_ssh_agent_auth/v0.10.2/pam_ssh_agent_auth-0.10.2.tar.bz2 # tar -xvjf pam_ssh_agent_auth-0.10.2.tar.bz2 # cd pam_ssh_agent_auth-0.10.2 # ./configure --libexecdir=/lib/security --with-mantype=man # make # checkinstall Note that the libexecdir option to the configure script is set since apparently Debian keeps PAM modules in a different place than pam_ssh_agent_auth expects by default. Configuration Edit the file /etc/pam.d/sudo and add the following line before any other auth or @include commands: auth sufficient pam_ssh_agent_auth.so file=~/.ssh/authorized_keys Run visudo to edit /etc/sudoers and add this line before any other Defaults lines: Defaults env_keep += SSH_AUTH_SOCK Invoking sudo To actually be able to use sudo now, run ssh-agent like so: $ eval `ssh-agent` and add the key: $ ssh-add -t 600 This will set the keys to timeout in 10 minutes (600 seconds). TODO A more elegant way of adding keys and running ssh-agent , including checking to see if a process is already running! References How to allow authentication with sudo using an alternate password? Using SSH agent for sudo authentication Using ssh-agent with ssh","title":"ssh-agent for sudo authentication with a passwordless account"},{"location":"blog/sudo-ssh-auth.html#prerequisites","text":"You'll want to start by ensuring you have generated ssh keys for your user and are using ssh-agent . To generate the keys: $ ssh-keygen Then just accept the defaults, but make sure to set a password for your new key pair. Add the public key to $HOME/.ssh/authorized_keys .","title":"Prerequisites"},{"location":"blog/sudo-ssh-auth.html#installation","text":"Since the PAM module isn't in Debian, first grab the build dependencies: # apt-get install build-essential checkinstall libssl-dev libpam0g-dev Next, grab the source and build: # wget http://downloads.sourceforge.net/project/pamsshagentauth/pam_ssh_agent_auth/v0.10.2/pam_ssh_agent_auth-0.10.2.tar.bz2 # tar -xvjf pam_ssh_agent_auth-0.10.2.tar.bz2 # cd pam_ssh_agent_auth-0.10.2 # ./configure --libexecdir=/lib/security --with-mantype=man # make # checkinstall Note that the libexecdir option to the configure script is set since apparently Debian keeps PAM modules in a different place than pam_ssh_agent_auth expects by default.","title":"Installation"},{"location":"blog/sudo-ssh-auth.html#configuration","text":"Edit the file /etc/pam.d/sudo and add the following line before any other auth or @include commands: auth sufficient pam_ssh_agent_auth.so file=~/.ssh/authorized_keys Run visudo to edit /etc/sudoers and add this line before any other Defaults lines: Defaults env_keep += SSH_AUTH_SOCK","title":"Configuration"},{"location":"blog/sudo-ssh-auth.html#invoking-sudo","text":"To actually be able to use sudo now, run ssh-agent like so: $ eval `ssh-agent` and add the key: $ ssh-add -t 600 This will set the keys to timeout in 10 minutes (600 seconds).","title":"Invoking sudo"},{"location":"blog/sudo-ssh-auth.html#todo","text":"A more elegant way of adding keys and running ssh-agent , including checking to see if a process is already running!","title":"TODO"},{"location":"blog/sudo-ssh-auth.html#references","text":"How to allow authentication with sudo using an alternate password? Using SSH agent for sudo authentication Using ssh-agent with ssh","title":"References"},{"location":"blog/tornado-background-tasks.html","text":"I have been using Tornado lately for distributed control of devices in the lab where an asynchronous framework is advantageous. In particular, we have a HighFinesse wavelength meter which we use to monitor and stabilize several lasers (up to 14 at a time). Previously, a custom server for controlling this wavemeter was written using Twisted , but that has proven difficult to upgrade, distribute, and maintain. One thing that is common for such a control scenario is that data needs to be refreshed continuously while still allowing incoming connections from clients and appropriately executing remote procedure calls. One method would be to periodically interrupt the Tornado IO loop to refresh data (and in fact, Tornado has a class to make this easy for you in tornado.ioloop.PeriodicCallback ). This can be fine if the data refreshing does not take too much time, but all other operations will be blocked until the callback is finished, which can be a problem if the refreshing operation is slow. Another option is to have an additional thread separate from the Tornado IO loop that handles refreshing data. This certainly works, but adds the complexity of needing to use thread-safe communications to stop the thread when the main application is shut down or when other tasks depend on the successful completion of the refresh. Luckily, Tornado also includes a decorator, tornado.concurrent.run_on_executor , to run things in the background for you using Python's concurrent.futures module (which is standard starting in Python 3.3 and backported for other versions). Then instead of writing the refresh function as a loop that runs in the background, you instead have its final call be to add itself back as a callback on the IO loop. This makes shutdown trivial since only the IO loop needs to be stopped when the program is closed. A refresh function could thus look something like this: @tornado.concurrent.run_on_executor def refresh(): do_something_that_takes_a_while() tornado.ioloop.IOLoop.instance().add_callback(self.refresh) Now after refresh is called once, it will continuously run until the IO loop is stopped. For a more complete example, I have written a small demo .","title":"Background tasks with Tornado"}]}